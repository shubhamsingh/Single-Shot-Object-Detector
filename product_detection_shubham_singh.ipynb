{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference to absolute path where compress folder extracted\n",
    "absolute_path = %pwd\n",
    "absolute_path =  absolute_path + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing output file so that don't get any error at runtime\n",
    "\n",
    "import os\n",
    "try:\n",
    "    os.remove(f'{absolute_path}{\"pack_detector/image2products.json\"}')\n",
    "    os.remove(f'{absolute_path}{\"pack_detector/tmp.json\"}')\n",
    "    os.remove(f'{absolute_path}{\"pack_detector/metrics.json\"}')\n",
    "    print(\"All Files are Removed!\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#All imports are required for this project\n",
    "\n",
    "import cv2\n",
    "import itertools\n",
    "import json\n",
    "import keras\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from collections import namedtuple, OrderedDict\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = absolute_path\n",
    "shelf_images = f'{absolute_path}{\"ShelfImages/\"}'\n",
    "product_images = f'{absolute_path}{\"GroceryDataset_part2/ProductImagesFromShelves/\"}'\n",
    " \n",
    "PATH_TO_MODEL = f'{absolute_path}{\"pack_detector/pack_detector_fg/frozen_inference_graph.pb\"}'\n",
    "PATH_TO_LABELS = f'{absolute_path}{\"pack_detector/data/pack.pbtxt\"}'\n",
    "PATH_TO_IMAGES = f'{absolute_path}{\"ShelfImages/test/\"}'\n",
    "PATH_TO_DATA = absolute_path\n",
    "PATH_TO_ANNOTATION = f'{absolute_path}{\"pack_detector/annotation.txt\"}'\n",
    "shelf_images_train = f'{absolute_path}{\"ShelfImages/train/\"}'\n",
    "shelf_images_test = f'{absolute_path}{\"ShelfImages/test/\"}'\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "img_path = f'{absolute_path}{\"ShelfImages/\"}'\n",
    "cropped_path = f'{absolute_path}{\"pack_detector/models/ssd_mobilenet_v1/\"}'\n",
    "detector_data_path = f'{absolute_path}{\"pack_detector/data/\"}'\n",
    "output_image_path = f'{absolute_path}{\"pack_detector/Output/\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[file            283\n",
       " shelf_id        283\n",
       " planogram_id    283\n",
       " dtype: int64,\n",
       " file            71\n",
       " shelf_id        71\n",
       " planogram_id    71\n",
       " dtype: int64]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining train and test sets as mentioned in assignment\n",
    "#And train:set is counts 283:71 \n",
    "files = [f for f in os.listdir(f'{shelf_images}{\"train\"}') if f.endswith('JPG')]\n",
    "train_df = pd.DataFrame([[f, f[:6], f[7:14]] for f in files], \n",
    "                         columns=['file', 'shelf_id', 'planogram_id'])\n",
    "files = [f for f in os.listdir(f'{shelf_images}{\"test\"}') if f.endswith('JPG')]\n",
    "test_df = pd.DataFrame([[f, f[:6], f[7:14]] for f in files], \n",
    "                         columns=['file', 'shelf_id', 'planogram_id'])\n",
    "[train_df.count(),test_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>shelf_id</th>\n",
       "      <th>planogram_id</th>\n",
       "      <th>category</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C4_P04_N3_S3_1.JPG</td>\n",
       "      <td>C4_P04</td>\n",
       "      <td>N3_S3_1</td>\n",
       "      <td>0</td>\n",
       "      <td>556</td>\n",
       "      <td>108</td>\n",
       "      <td>224</td>\n",
       "      <td>364</td>\n",
       "      <td>780</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1_P12_N3_S4_1.JPG</td>\n",
       "      <td>C1_P12</td>\n",
       "      <td>N3_S4_1</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2004</td>\n",
       "      <td>216</td>\n",
       "      <td>332</td>\n",
       "      <td>308</td>\n",
       "      <td>2336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C4_P06_N1_S4_1.JPG</td>\n",
       "      <td>C4_P06</td>\n",
       "      <td>N1_S4_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2316</td>\n",
       "      <td>764</td>\n",
       "      <td>216</td>\n",
       "      <td>312</td>\n",
       "      <td>2532</td>\n",
       "      <td>1076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C3_P01_N2_S2_1.JPG</td>\n",
       "      <td>C3_P01</td>\n",
       "      <td>N2_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2472</td>\n",
       "      <td>1524</td>\n",
       "      <td>352</td>\n",
       "      <td>584</td>\n",
       "      <td>2824</td>\n",
       "      <td>2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C2_P07_N2_S2_1.JPG</td>\n",
       "      <td>C2_P07</td>\n",
       "      <td>N2_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>468</td>\n",
       "      <td>996</td>\n",
       "      <td>244</td>\n",
       "      <td>388</td>\n",
       "      <td>712</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file shelf_id planogram_id  category  xmin  ymin    w    h  \\\n",
       "0  C4_P04_N3_S3_1.JPG   C4_P04      N3_S3_1         0   556   108  224  364   \n",
       "1  C1_P12_N3_S4_1.JPG   C1_P12      N3_S4_1         0    92  2004  216  332   \n",
       "2  C4_P06_N1_S4_1.JPG   C4_P06      N1_S4_1         0  2316   764  216  312   \n",
       "3  C3_P01_N2_S2_1.JPG   C3_P01      N2_S2_1         0  2472  1524  352  584   \n",
       "4  C2_P07_N2_S2_1.JPG   C2_P07      N2_S2_1         0   468   996  244  388   \n",
       "\n",
       "   xmax  ymax  \n",
       "0   780   472  \n",
       "1   308  2336  \n",
       "2  2532  1076  \n",
       "3  2824  2108  \n",
       "4   712  1384  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df = pd.DataFrame(\n",
    "    [[f[:18], f[:6], f[7:14], i, *map(int, f[19:-4].split('_'))] \n",
    "     for i in range(11) \n",
    "     for f in os.listdir(f'{product_images}{i}') if f.endswith('png')],\n",
    "    columns=['file', 'shelf_id', 'planogram_id', \n",
    "             'category', 'xmin', 'ymin', 'w', 'h'])\n",
    "products_df['xmax'] = products_df['xmin'] + products_df['w']\n",
    "products_df['ymax'] = products_df['ymin'] + products_df['h']\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelves = list(set(train_df['shelf_id'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brand Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelves_train, shelves_validation, _, _ = train_test_split(\n",
    "    shelves, shelves, test_size=0.3, random_state=6)\n",
    "\n",
    "def train(shelf_id): return True\n",
    "def test(shelf_id): return False\n",
    "def is_train(shelf_id): return shelf_id in shelves_train\n",
    "\n",
    "train_df['is_train'] = train_df.shelf_id.apply(train)\n",
    "test_df['is_train'] = test_df.shelf_id.apply(test)\n",
    "training_df = train_df.append(test_df)\n",
    "products_df['is_train'] = products_df.shelf_id.apply(is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for using neural network, I have to fix the image size. \n",
    "num_classes = 10\n",
    "SHAPE_WIDTH = 80\n",
    "SHAPE_HEIGHT = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img):\n",
    "    fx_ratio = SHAPE_WIDTH / img.shape[1]\n",
    "    fy_ratio = SHAPE_HEIGHT / img.shape[0]    \n",
    "    img = cv2.resize(img, (0, 0), fx=fx_ratio, fy=fy_ratio)\n",
    "    return img[0:SHAPE_HEIGHT, 0:SHAPE_WIDTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x - image, y - class, f - is_train flag\n",
    "x, y, f = [], [], []\n",
    "for file, is_train in training_df[['file', 'is_train']].values:\n",
    "    photo_rects = products_df[products_df.file == file]\n",
    "    rects_data = photo_rects[['category', 'xmin', 'ymin', 'xmax', 'ymax']]\n",
    "    if is_train == True:\n",
    "        shelf_images = shelf_images_train\n",
    "    else:\n",
    "        shelf_images = shelf_images_test\n",
    "    im = cv2.imread(f'{shelf_images}{file}')\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    for category, xmin, ymin, xmax, ymax in rects_data.values:\n",
    "        if category == 0:\n",
    "            continue\n",
    "        img = resize_image(np.array(im[ymin:ymax, xmin:xmax]))\n",
    "        x.append(img)\n",
    "        f.append(is_train)\n",
    "        y.append(category - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "f = np.array(f)\n",
    "x_train, x_validation, y_train, y_validation = x[f], x[~f], y[f], y[~f]\n",
    "x_validation_images = x_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_train and y_validation to one-hot arrays\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_validation = keras.utils.to_categorical(y_validation, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "x_train = x_train.astype('float32')\n",
    "x_validation = x_validation.astype('float32')\n",
    "x_train /= 255\n",
    "x_validation /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (2119, 120, 80, 3)\n",
      "y_train shape: (2119, 10)\n",
      "2119 train samples\n",
      "625 validation samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_validation.shape[0], 'validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shubhamsingh/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "# Building ResNet CNN.\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 5:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=x_train.shape[1:])\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "n = 3\n",
    "version = 1\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "model = resnet_v1(input_shape=x_train.shape[1:], depth=depth, num_classes=num_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 120, 80, 3)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 120, 80, 16)  448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 120, 80, 16)  64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 120, 80, 16)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 120, 80, 16)  2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 120, 80, 16)  64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 120, 80, 16)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 120, 80, 16)  2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 120, 80, 16)  64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 120, 80, 16)  0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 120, 80, 16)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 120, 80, 16)  2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 120, 80, 16)  64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 120, 80, 16)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 120, 80, 16)  2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 120, 80, 16)  64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 120, 80, 16)  0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 120, 80, 16)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 120, 80, 16)  2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 120, 80, 16)  64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 120, 80, 16)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 120, 80, 16)  2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 120, 80, 16)  64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 120, 80, 16)  0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 120, 80, 16)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 60, 40, 32)   4640        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 60, 40, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 60, 40, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 60, 40, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 60, 40, 32)   544         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 60, 40, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 60, 40, 32)   0           conv2d_10[0][0]                  \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 60, 40, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 60, 40, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 60, 40, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 60, 40, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 60, 40, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 60, 40, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 60, 40, 32)   0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 60, 40, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 60, 40, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 60, 40, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 60, 40, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 60, 40, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 60, 40, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 60, 40, 32)   0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 60, 40, 32)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 30, 20, 64)   18496       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 30, 20, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 30, 20, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 30, 20, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 30, 20, 64)   2112        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 30, 20, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 30, 20, 64)   0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 30, 20, 64)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 30, 20, 64)   36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 30, 20, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 30, 20, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 30, 20, 64)   36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 30, 20, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 30, 20, 64)   0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 30, 20, 64)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 30, 20, 64)   36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 30, 20, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 30, 20, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 30, 20, 64)   36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 30, 20, 64)   256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 30, 20, 64)   0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 30, 20, 64)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 3, 2, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           3850        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 277,642\n",
      "Trainable params: 276,266\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True)  # randomly flip images\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shubhamsingh/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/15\n",
      "Learning rate:  0.001\n",
      "43/43 [==============================] - 255s 6s/step - loss: 1.8201 - accuracy: 0.5021 - val_loss: 3.4674 - val_accuracy: 0.1312\n",
      "Epoch 2/15\n",
      "Learning rate:  0.001\n",
      "43/43 [==============================] - 259s 6s/step - loss: 0.9617 - accuracy: 0.7471 - val_loss: 6.8243 - val_accuracy: 0.1232\n",
      "Epoch 3/15\n",
      "Learning rate:  0.001\n",
      "43/43 [==============================] - 262s 6s/step - loss: 0.6791 - accuracy: 0.8429 - val_loss: 2.3942 - val_accuracy: 0.5248\n",
      "Epoch 4/15\n",
      "Learning rate:  0.001\n",
      "43/43 [==============================] - 276s 6s/step - loss: 0.5813 - accuracy: 0.8735 - val_loss: 1.2416 - val_accuracy: 0.7056\n",
      "Epoch 5/15\n",
      "Learning rate:  0.001\n",
      "43/43 [==============================] - 274s 6s/step - loss: 0.4632 - accuracy: 0.9094 - val_loss: 1.2089 - val_accuracy: 0.7328\n",
      "Epoch 6/15\n",
      "Learning rate:  0.001\n",
      "43/43 [==============================] - 278s 6s/step - loss: 0.3981 - accuracy: 0.9363 - val_loss: 1.7069 - val_accuracy: 0.5968\n",
      "Epoch 7/15\n",
      "Learning rate:  0.0001\n",
      "43/43 [==============================] - 271s 6s/step - loss: 0.3371 - accuracy: 0.9533 - val_loss: 0.8870 - val_accuracy: 0.8048\n",
      "Epoch 8/15\n",
      "Learning rate:  0.0001\n",
      "43/43 [==============================] - 230s 5s/step - loss: 0.3162 - accuracy: 0.9538 - val_loss: 0.7836 - val_accuracy: 0.8384\n",
      "Epoch 9/15\n",
      "Learning rate:  0.0001\n",
      "43/43 [==============================] - 229s 5s/step - loss: 0.3077 - accuracy: 0.9613 - val_loss: 0.7367 - val_accuracy: 0.8464\n",
      "Epoch 10/15\n",
      "Learning rate:  0.0001\n",
      "43/43 [==============================] - 229s 5s/step - loss: 0.3021 - accuracy: 0.9613 - val_loss: 0.7229 - val_accuracy: 0.8672\n",
      "Epoch 11/15\n",
      "Learning rate:  0.0001\n",
      "43/43 [==============================] - 227s 5s/step - loss: 0.2828 - accuracy: 0.9679 - val_loss: 0.7048 - val_accuracy: 0.8736\n",
      "Epoch 12/15\n",
      "Learning rate:  0.0001\n",
      "43/43 [==============================] - 227s 5s/step - loss: 0.2793 - accuracy: 0.9665 - val_loss: 0.7133 - val_accuracy: 0.8736\n",
      "Epoch 13/15\n",
      "Learning rate:  0.0001\n",
      "43/43 [==============================] - 228s 5s/step - loss: 0.2825 - accuracy: 0.9674 - val_loss: 0.6800 - val_accuracy: 0.8784\n",
      "Epoch 14/15\n",
      "Learning rate:  0.0001\n",
      "43/43 [==============================] - 236s 5s/step - loss: 0.2792 - accuracy: 0.9632 - val_loss: 0.7031 - val_accuracy: 0.8832\n",
      "Epoch 15/15\n",
      "Learning rate:  0.0001\n",
      "43/43 [==============================] - 229s 5s/step - loss: 0.2659 - accuracy: 0.9731 - val_loss: 0.7220 - val_accuracy: 0.8832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a6a338790>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 15\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    validation_data=(x_validation, y_validation),\n",
    "                    epochs=epochs, verbose=1, workers=4, \n",
    "                    callbacks=[LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "model.save(f'{data_path}{\"pack_detector/\"}{\"model.h5\"}')\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "model_loaded = load_model(f'{data_path}{\"pack_detector/\"}{\"model.h5\"}')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 16s 26ms/step\n",
      "Test loss: 0.7220046576499939\n",
      "Test accuracy: 0.8831999897956848\n"
     ]
    }
   ],
   "source": [
    "scores = model_loaded.evaluate(x_validation, y_validation, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, \n",
    "                          title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix to check classes recognition performance\n",
    "y_validation_cls = np.argmax(y_validation, axis=1)\n",
    "y_validation_predict = model.predict(x_validation)\n",
    "y_validation_predict_cls = np.argmax(y_validation_predict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation\n",
    "y_validation_score = []\n",
    "for row in y_validation_predict:\n",
    "    ind = np.argmax(row)\n",
    "    row = np.array((0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))\n",
    "    row[ind] = row[ind]+1\n",
    "    y_validation_score.append(row)\n",
    "y_validation_score = np.asarray(y_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAALICAYAAABYe7kFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXwV5dn4/88NEcQFEgSFJFA2ZYlFka2ouNRaFxa1iliVSrXap3XvYqv259o+brUu1W+tVqtVK4gbgopYKT4uVRZ3wAUFlARckMWNCIf798cJMSELsSE5k5PP+/U6L5mZe+ZcF3M4XrlzzUyIMSJJkiQlUYtMByBJkiTVxGJVkiRJiWWxKkmSpMSyWJUkSVJiWaxKkiQpsSxWJUmSlFgWq5IkSaq3EMJtIYQPQwiv17A9hBCuDyEsDCG8GkLYoy7HtViVJEnSlnA7cHAt2w8Bdi57nQL8pS4HtViVJElSvcUY/w/4pJYhhwH/iGnPA7khhM6bO27OlgpQkiRJW17Ltt+Kcf2XmQ6D+OVH84C1FVbdHGO8+RscogB4v8Ly0rJ1y2rbyWJVkiQpweL6L2nd++hMh8Hal29cG2McVI9DhGrWxc3tZBuAJEmSGsNSoEuF5UKgZHM7WaxKkiSpMTwM/KjsrgDfAVbHGGttAQDbACRJkhIuQEj+/GII4R5gP6BDCGEpcCGwFUCM8SbgUeBQYCHwBfDjuhzXYlWSJEn1FmP84Wa2R+DUb3pci1VJkqQkC0Co7tqk5iH5c8qSJElqtixWJUmSlFi2AUiSJCVdE7jAqqE038wlSZKUeBarkiRJSizbACRJkpLOuwFIkiRJyePMqiRJUqI1jSdYNZTmm7kkSZISz2JVkiRJiWUbgCRJUtJ5gZUkSZKUPBarkiRJSizbACRJkpIs4N0AJEmSpCRyZlWSJCnRghdYSZIkSUlksSpJkqTEsg1AkiQp6bzASpIkSUoei1VJkiQllm0AkiRJSefdACRJkqTkcWZVkiQp0YIXWEmSJElJZLEqSZKkxLINQJIkKckCXmAlSZIkJZHFqiRJkhLLNgBJkqSk824AkiRJUvJYrEqSJCmxbAOQJElKNB8KIEmSJCWSM6uSJElJ18L7rEqSJEmJY7EqSZKkxLINQJIkKckCXmAlSZIkJZHFqiRJkhLLNgBJkqSkC94NQJIkSUocZ1YlSZISzSdYSZIkSYlksSpJkqTEsg1AkiQp6bzASpIkSUoei1VJkiQllm0AkiRJSefdACRJkqTkcWZVkiQpyULwAitJkiQpiSxWJUmSlFi2AUiSJCWdF1hJUmaEENqEEKaEEFaHECbV4zjHhRCmb8nYMiWEMDyE8Gam45CkJLBYlVQnIYRjQwhzQgifhRCWhRAeCyHsvQUOfRSwE7BDjHHMf3uQGOPdMcbvb4F4GlQIIYYQetU2Jsb4dIyxd2PFJElJZhuApM0KIfwC+C3wP8DjwFfAwcBhwDP1PPy3gLdijOvreZysEELI8e9CUhXeDUCSqhdCaAdcApwaY3wgxvh5jHFdjHFKjPHXZWNahxCuDSGUlL2uDSG0Ltu2XwhhaQjhlyGED8tmZX9ctu1i4AJgbNmM7UkhhItCCHdVeP9uZbOROWXL40MI74YQPg0hLAohHFdh/TMV9tszhDC7rL1gdghhzwrbZoYQLg0hPFt2nOkhhA415L8x/nMqxH94COHQEMJbIYRPQgjnVRg/JITwnxDCqrKxN4QQWpVt+7+yYa+U5Tu2wvF/E0JYDvx947qyfXqWvcceZcv5IYSPQwj71evESlITYbEqaXOGAVsDD9Yy5nzgO8DuwG7AEOB3FbZ3AtoBBcBJwI0hhLwY44XA/wITY4zbxRhvrS2QEMK2wPXAITHG7YE9gZerGdceeKRs7A7An4BHQgg7VBh2LPBjYEegFfCrWt66E+m/gwLSxfUtwPHAQGA4cEEIoUfZ2BRwNtCB9N/dAcDPAWKM+5SN2a0s34kVjt+e9CzzKRXfOMb4DvAb4O4QwjbA34HbY4wza4lXUlYJ6QusMv3KEItVSZuzA/DxZn41fRxwSYzxwxjjR8DFwLgK29eVbV8XY3wU+Az4b3syNwC7hhDaxBiXxRjnVTNmBPB2jPHOGOP6GOM9wBvAqApj/h5jfCvG+CVwL+lCuybrgD/EGNcBE0gXotfFGD8te/95QH+AGOPcGOPzZe+7GPgrsG8dcrowxlhaFk8lMcZbgLeBF4DOpH84kKRmwWJV0uasADps/DV8DfKBJRWWl5StKz/GJsXuF8B23zSQGOPnwFjSvbPLQgiPhBD61CGejTEVVFhe/g3iWRFjTJX9eWMx+UGF7V9u3D+EsEsIYWoIYXkIYQ3pmeNqWwwq+CjGuHYzY24BdgX+HGMs3cxYScoaFquSNuc/wFrg8FrGlJD+FfZGXcvW/Tc+B7apsNyp4sYY4+MxxgNJzzC+QbqI21w8G2Mq/i9j+ib+QjqunWOMbYHzgM1dGRFr2xhC2A64FrgVuKiszUFSc7LxkauZfGWIxaqkWsUYV5Pu07yx7MKibUIIW4UQDgkhXFk27B7gdyGEjmUXKl0A3FXTMTfjZWCfEELXsou7zt24IYSwUwhhdFnvainpdoJUNcd4FNil7HZbOSGEsUA/YOp/GdM3sT2wBvisbNb3Z5ts/wDoUWWv2l0HzI0x/oR0L+5N9Y5SkpoIi1VJmxVj/BPwC9IXTX0EvA+cBjxUNuT3wBzgVeA14MWydf/Nez0BTCw71lwqF5gtgF+Snjn9hHQv6M+rOcYKYGTZ2BXAOcDIGOPH/01M39CvSF+89SnpWd+Jm2y/CLij7G4BR2/uYCGEw0jfJux/ylb9Athj410QJCnbhRhr/e2TJEmSMqhFbtfYeu9zMh0Gax85fW6McVBjv68zq5IkSUosi1VJkiQllo9blSRJSrSQ0ZvyZ1rzzVySJEmJ1+RnVkNOmxhabZ/pMLa4AX27ZjoESZKatSVLFvPxxx9n7gajFWXwPqeZ1vSL1Vbb07r3Zu/+0uQ8+8INmQ5BkqRmba+hjX7hu6phG4AkSZISq8nPrEqSJGU9L7CSJEmSksdiVZIkSYllG4AkSVLSNeO7ATizKkmSpMRyZlWSJCnJgk+wkiRJkhLJYlWSJEmJZRuAJElS0nmBlSRJkpQ8FquSJElKLNsAJEmSEi7YBiBJkiQljzOrkiRJCRZwZlWSJElKJItVSZIkJZZtAJIkSUkWyl7NlDOrwE0XHseSJy9jzqTzahxz9TlH8frkC5k18Vx271NYvv64UUN5bfIFvDb5Ao4bNbQxwq2z6Y9Po39Rb4r69OKqKy+vsr20tJTjjx1LUZ9eDN9zKEsWLy7fdtUVl1HUpxf9i3rzxPTHGzHqzcvGvLIxJzAv88q8bMwJzKup5aX6sVgF7pzyPIedemON2w/aux89u3Zk18Mu5rTf38P15x0DQF7bbTj/lEPYZ9wfGX78VZx/yiHkbt+mscKuVSqV4qwzTmXylMd46dX5TJpwDwvmz6805vbbbiUvN495byzk9DPP5vzzfgPAgvnzmTRxAi++Mo+Hp07jzNN/TiqVykQaVWRjXtmYE5iXeWU+r2zMCcyrqeWl+rNYBZ598R0+Wf1FjdtH7tuff06dBcCs1xbTbvs2dOrQlgP37MuTz7/ByjVfsOrTL3ny+Tf4/l79GivsWs2eNYuePXvRvUcPWrVqxZixxzB1yuRKY6ZOmcxx404A4AdHHsXMGU8SY2TqlMmMGXsMrVu3plv37vTs2YvZs2ZlIo0qsjGvbMwJzMu8Mp9XNuYE5tXU8toyAiFk/pUpFqt1kL9jLkuXryxfLv5gFfk75pLfMZelH1RY/+Eq8jvmZiLEKkpKiiks7FK+XFBQSHFxcdUxXdJjcnJyaNuuHStWrKC4uOq+JSWV982UbMwrG3MC8zKvzOeVjTmBeTW1vFR/GSlWQwidQggTQgjvhBDmhxAeDSHsEkKYFkJYFUKYmom4alLdDxMxxurXExs+oDqIsWocm/5UVOOYOuybKdmYVzbmBOZV7RjzalTZmBOYV7VjEpzXlpLpWdVmNbMa0tk+CMyMMfaMMfYDzgN2Aq4CxjV2TJtT/MEqCjvllS8X7JTLso9WU/zhKgp3qrB+x/T6JCgoKGTp0vfLl4uLl5Kfn191zPvpMevXr2fN6tW0b9+egsKq+3buXHnfTMnGvLIxJzAv88p8XtmYE5hXU8tL9ZeJmdX9gXUxxps2rogxvhxjfDrG+CTwaQZiqtUjT73GsSOHADDk291Y89mXLP94DU88t4DvDetD7vZtyN2+Dd8b1ocnnluQ4WjTBg0ezMKFb7N40SK++uorJk2cwIiRoyuNGTFyNHffeQcAD9x/H/vu/11CCIwYOZpJEydQWlrK4kWLWLjwbQYPGZKJNKrIxryyMScwL/PKfF7ZmBOYV1PLS/WXifus7grMrc8BQginAKcAsNV29Q7ojsvGM3zgznTI3Y6F0y7l0pseZauclgD87b5nmPbMPA7au4h5D1/IF2vX8dOL7gJg5ZovuOyWaTxz1zkA/O/N01i5puYLtRpTTk4O11x3A6NGHEQqleKE8SfSr6iISy66gD0GDmLkqNGMP/EkThw/jqI+vcjLa8+dd08AoF9REUeOOZoB/fuRk5PDtdffSMuWLTOcUVo25pWNOYF5mVfm88rGnMC8mlpeW0q2tTV8E6G6/o8GfcMQzgC6xxjPrmH7fsCvYowj63K8FtvsGFv3PnoLRpgMK2ffkOkQJElq1vYaOoi5c+dkvEps2b573Pb7F2c6DD6deMLcGOOgxn7fTLQBzAMGZuB9JUmS1MRkolidAbQOIZy8cUUIYXAIYd8MxCJJkpR4mb4TQLO6G0BM9x0cARxYduuqecBFQEkI4WlgEnBACGFpCOGgxo5PkiRJyZGJC6yIMZYA1TWaDm/sWCRJkhItlL2aKZ9gJUmSpMSyWJUkSVJiZaQNQJIkSXUTyOwFTpnmzKokSZISy2JVkiRJiWUbgCRJUsLZBiBJkiQlkMWqJEmSEss2AEmSpISzDUCSJElKIGdWJUmSEs6ZVUmSJCmBLFYlSZKUWLYBSJIkJVkoezVTzqxKkiQpsSxWJUmSlFi2AUiSJCWcdwOQJEmSEsiZVUmSpAQLBGdWJUmSpCRq8jOrA/p25dkXbsh0GFtc3uDTMh1Cg1g5O/vOlSRJajhNvliVJEnKdrYBSJIkSQlksSpJkqTEsg1AkiQp6ZpvF4Azq5IkSUoui1VJkqQkC+kLrDL9qlOoIRwcQngzhLAwhPDbarZ3DSH8O4TwUgjh1RDCoZs7psWqJEmS6i2E0BK4ETgE6Af8MITQb5NhvwPujTEOAI4B/t/mjmuxKkmSpC1hCLAwxvhujPErYAJw2CZjItC27M/tgJLNHdQLrCRJkhIuIfdZ7RBCmFNh+eYY480VlguA9yssLwWGbnKMi4DpIYTTgW2B723uTS1WJUmSVBcfxxgH1bK9uoo6brL8Q+D2GOPVIYRhwJ0hhF1jjBtqOqhtAJIkSdoSlgJdKiwXUvXX/CcB9wLEGP8DbA10qO2gFquSJEkJl+k7AdSxDWE2sHMIoXsIoRXpC6ge3mTMe8ABZTn1JV2sflTbQS1WJUmSVG8xxvXAacDjwALSV/3PCyFcEkIYXTbsl8DJIYRXgHuA8THGTVsFKrFnVZIkKcECdb/PaabFGB8FHt1k3QUV/jwf2OubHNOZVUmSJCWWxaokSZISyzYASZKkpGsaXQANwplVSZIkJZbFqiRJkhLLYhWY/vg0+hf1pqhPL6668vIq20tLSzn+2LEU9enF8D2HsmTx4vJtV11xGUV9etG/qDdPTH+8EaOu3U0XHseSJy9jzqTzahxz9TlH8frkC5k18Vx271NYvv64UUN5bfIFvDb5Ao4btelT0jIvG89XNuYE5mVemZeNOYF5NbW86i00mfusNohmX6ymUinOOuNUJk95jJdenc+kCfewYP78SmNuv+1W8nLzmPfGQk4/82zOP+83ACyYP59JEyfw4ivzeHjqNM48/eekUqlMpFHFnVOe57BTb6xx+0F796Nn147setjFnPb7e7j+vGMAyGu7Deefcgj7jPsjw4+/ivNPOYTc7ds0VtiblY3nKxtzAvMyr8znlY05gXk1tbxUf82+WJ09axY9e/aie48etGrVijFjj2HqlMmVxkydMpnjxp0AwA+OPIqZM54kxsjUKZMZM/YYWrduTbfu3enZsxezZ83KRBpVPPviO3yy+osat4/ctz//nJqOddZri2m3fRs6dWjLgXv25cnn32Dlmi9Y9emXPPn8G3x/r36NFfZmZeP5ysacwLzMK/N5ZWNOYF5NLa8tJdOzqs6sZlBJSTGFhV8/xragoJDi4uKqY7qkx+Tk5NC2XTtWrFhBcXHVfUtKKu+bVPk75rJ0+cry5eIPVpG/Yy75HXNZ+kGF9R+uIr9jbiZCrFY2nq9szAnMy7wyn1c25gTm1dTyUv1lpFgNIXQKIUwIIbwTQpgfQng0hDAkhPCfEMK8EMKrIYSxjRFLdU/42vSnhxrH1GHfpKouzBhj9eup9SlojSobz1c25gTmVe0Y82pU2ZgTmFe1YxKcl+qv0YvVkP70PAjMjDH2jDH2A84D2gA/ijEWAQcD14YQGnxKr6CgkKVL3y9fLi5eSn5+ftUx76fHrF+/njWrV9O+fXsKCqvu27lz5X2TqviDVRR2yitfLtgpl2Ufrab4w1UU7lRh/Y7p9UmRjecrG3MC8zKvzOeVjTmBeTW1vLaUTLcANLc2gP2BdTHGmzauiDG+HGN8Ksb4dtlyCfAh0LGhgxk0eDALF77N4kWL+Oqrr5g0cQIjRo6uNGbEyNHcfecdADxw/33su/93CSEwYuRoJk2cQGlpKYsXLWLhwrcZPGRIQ4e8RTzy1GscOzId65Bvd2PNZ1+y/OM1PPHcAr43rA+527chd/s2fG9YH554bkGGo/1aNp6vbMwJzMu8Mp9XNuYE5tXU8lL9ZeIJVrsCc2sbEEIYArQC3qlh+ynAKQBdunatVzA5OTlcc90NjBpxEKlUihPGn0i/oiIuuegC9hg4iJGjRjP+xJM4cfw4ivr0Ii+vPXfePQGAfkVFHDnmaAb070dOTg7XXn8jLVu2rFc8W8odl41n+MCd6ZC7HQunXcqlNz3KVjnp2P523zNMe2YeB+1dxLyHL+SLtev46UV3AbByzRdcdss0nrnrHAD+9+ZprFxT84VajS0bz1c25gTmZV6ZzysbcwLzamp5qf5Cdf0fDfqGIZwBdI8xnl3D9s7ATOCEGOPzmzvewIGD4rMvzNmyQSZA3uDTMh1Cg1g5+4ZMhyBJUp3sNXQQc+fOyXjza6sde8Wdjr4602Gw9MbD58YYBzX2+2aiDWAeMLC6DSGEtsAjwO/qUqhKkiQpu2WiWJ0BtA4hnLxxRQhhcAhhX9IXXv0jxjgpA3FJkiQpYRq9ZzXGGEMIR5C+2v+3wFpgMfA8sA+wQwhhfNnw8THGlxs7RkmSpCRpzrfiysQFVhuv9j+6mk2XNnYskiRJSq6MFKuSJEmqm0zf5zTTmv3jViVJkpRcFquSJElKLNsAJEmSEs42AEmSJCmBLFYlSZKUWLYBSJIkJZxtAJIkSVICObMqSZKUdM13YtWZVUmSJCWXxaokSZISyzYASZKkhPMCK0mSJCmBLFYlSZKUWLYBSJIkJVmwDUCSJElKJGdWJUmSEiwAzXhi1ZlVSZIkJZfFqiRJkhLLNgBJkqREC836AqsmX6ymYuSL0vWZDmOLWzjj6kyH0CCefvujTIewxQ3fuWOmQ5A4/h9zMx1Cg7jrRwMzHYKkDLMNQJIkSYnV5GdWJUmSsl0z7gJwZlWSJEnJ5cyqJElSwjXnC6ycWZUkSVJiWaxKkiQpsWwDkCRJSrLgBVaSJElSIlmsSpIkKbFsA5AkSUqwALRo0Xz7AJxZlSRJUmJZrEqSJCmxbAOQJElKOO8GIEmSJCWQM6uSJEkJ5+NWJUmSpASyWJUkSVJi2QYgSZKUZD5uVZIkSUomi1VJkiQllsUq8OQTjzNkQBGD+vfh2quvrLK9tLSUk350LIP69+HA/fbkvSWLAXhvyWIKOmzPvsMGsu+wgfzyjJ83cuS1+/e/prPPkG+z18B+3HDtVVW2P//c0xy833f4VsdtmTr5gUrbJt1zJ3sPKmLvQUVMuufOxgq5TmY/PYMTDx3G+IOGMOGW66tsv+/2v/CTkXvz08P35ZwfH8kHxe+Xb/vb1Zdw8uh9OHn0Psx87KHGDLtW0x+fRv+i3hT16cVVV15eZXtpaSnHHzuWoj69GL7nUJYsXly+7aorLqOoTy/6F/XmiemPN2LUm2deTSuv3Qvact2RRfz5qCIO779Tle379dqBW3/Yn6sO68tVh/XlgF12AKCo03bl6646rC///NEABndt19jhVytbz5V5Na286iuQvhtApl+Z0uyL1VQqxTm/OIN7H5jCc3Ne5YFJE3hjwfxKY+664zZyc3OZ8+ob/OzUM7n4/zuvfFu37j156j9zeeo/c7n6+v/X2OHXKJVK8btzzuTOeyfz7/+8zOT77+WtNxZUGlNQ2IU/3XgLhx81ttL6lSs/4Zor/8CUJ55m6r+e4Zor/8CqVSsbM/wapVIpbvj9b/jDX+/hlinPMPPRB1iy8M1KY3r1/TY3TJrOXx96iuEHjeRvV18CwAtPPcHb81/lpgdmcP2Ex5h02418/tmnmUijklQqxVlnnMrkKY/x0qvzmTThHhbMr/wZvP22W8nLzWPeGws5/cyzOf+83wCwYP58Jk2cwIuvzOPhqdM48/Sfk0qlMpFGFebVtPJqEeAnw7ryh+lvc/YD89m7R3sKc7euMu65RSv59eQF/HryAp58awUA85Z/Vr7u4sfeojS1gVeK1zR2ClVk67kyr6aVl+qv2RerL86ZRfcePenWvQetWrXiiKPG8tgjUyqNeeyRKRxz3DgARh9xJP83cwYxxkyEW2cvz51Nt+49+Va3dF6H/WAM0x+rnFeXrt3oV/RtWrSo/DF4asYTDN/vAPLy2pObm8fw/Q5g5pPTGzP8Gr352ovkd+1O5y7d2KpVK/Y95AiemzGt0pjdh+7N1m22AaBv/0F89EEJAEsWvkn/wXvSMieHNttsS4/eRcx5ekaj57Cp2bNm0bNnL7r3SJ+rMWOPYeqUyZXGTJ0ymePGnQDAD448ipkzniTGyNQpkxkz9hhat25Nt+7d6dmzF7NnzcpEGlWYV9PKq1eHbVm+Zi0ffvoV6zdEnn13JYO75n7j43ynex4vL13NV6nMf0dm67kyr6aV15aR+VlVZ1YzaFlJCQWFheXL+QUFLCsprjImv7ALADk5ObRt145PVqRnFN5bsoj99hzEqIO+y3+efabxAt+MZctK6FzwdV6d8gtYtqykTvsuLykhv8K+nfMLWF5St30b2scfLKdjp4Ly5Y6dOrPiw2U1jp/2wN0MHn4AAD36FDH76SdZ++UXrF65gldmPcNHy4tr3LexlJQUU1j2+QIoKCikuLi46pgulT+DK1asoLi46r4lJZnPCcyrqeXVftut+PjzdeXLKz7/ivbbbFVl3He65XH14X355f492GHbqtv36p7HM+8m4zcx2XquzKtp5aX6y8itq0IInYBrgcFAKbAYuAL4E9AS2Ar4c4zxpoaOpboZ0k1/eqhpzE6dOvPKgndpv8MOvPzSXMYdcxTPzn6Ftm3bNli8dVaHvGre9b/ft8FVFxvVx/avhyfx1uuv8Md/pHtTB+21P2+99jJnHTuCdu13oO9ug2iZk/m7t9XnM1if89zQzKuaMQnOq7ooNo12zvureObdT1i/IfL93h04bXg3Lp72dvn23DY5dM1rw8tLVzdorHWVrefKvKoZk+C8VH+NPrMa0p+eB4GZMcaeMcZ+wMYm0D1jjLsDQ4HfhhDyGzqe/IICipcuLV8uKS6mU+f8KmNKlqYv0lm/fj1rVq8mr317WrduTfsd0hcY7D5gIN279+CdhW81dMh10jm/gGXFX+e1vKSYTp06123fggJKKuy7rKSYnTrXbd+G1qFT50qzoR8tX0b7HTtVGffic09xz83XcvGN/6BVq9bl64/9n7O56cF/c8Wt9wFQ0LVHwwe9GQUFhSxd+vVFYMXFS8nPz6865v3Kn8H27dtTUFh1386dG/yfTZ2YV9PKa8Xn6+hQYaZ0h21bsfKLdZXGfFaaYv2GdFHwr7c+pkeHbStt37N7e2a9t4oEdAAA2XuuzKtp5bWlhJD5V6Zkog1gf2BdxVnTGOPLMcanYoylZataN1ZsAwYO5t13FrJk8SK++uorHrxvIoccOrLSmIMPHcmEu9NXxD/84P0M33d/Qgh8/NFH5Q3cixe9yzvvLKRbt8wXPwC77TGIRe8u5L0l6bwmPzCJAw8eufkdgX2/eyD/9+9/sWrVSlatWsn//ftf7PvdAxs44rrpvesAipe8y7KlS1j31Vc89diDDNv/oEpjFs5/jesu/hWX3HAneTt0LF+fSqVYs+oTAN59cx7vvjmfgXvt15jhV2vQ4MEsXPg2ixelz9WkiRMYMXJ0pTEjRo7m7jvvAOCB++9j3/2/SwiBESNHM2niBEpLS1m8aBELF77N4CFDMpFGFebVtPJa+PHndG63NTtu14qcFoG9euQx+71Vlcbktvn6NxGDuuZSvOrLStv37pHHM+980ijx1kW2nivzalp5qf4y8TvQXYG51W0IIXQBHgF6Ab+OMVbbKBlCOAU4BaCwS9d6BZOTk8MVV1/HmMNHkEqlOHbcePr0K+KySy9i9z0GcsiIURx/won87CfjGdS/D7l5efzt9rsBeO7Zp7n89xeTk9OSli1bcvV1N5LXvn294tlScnJyuPTKaznuqFFsSKUYe9wJ9O7bj6v+92J2GzCQ7x8ykpdfnMNPxo1l9eqVPDHtUf50+aXM+M9L5OW158xfncuIA/YC4Kxfn0deXjLyapmTw2nnX855J49lw4YUBx1xLN127sMdf76cXYp2Z9h3D+aWP17El198zqVnnwTAjvmFXHLjnaTWr+MXx6e/+LbZbnt+e8WNiWgDyMnJ4ZAeS0gAACAASURBVJrrbmDUiINIpVKcMP5E+hUVcclFF7DHwEGMHDWa8SeexInjx1HUpxd5ee258+4JAPQrKuLIMUczoH8/cnJyuPb6G2nZsmWGM0ozr6aV14YIf/vPe/zuoJ1pEQIz3v6YpavWMnZAZ975+AvmvL+aQ/vtyOCuuaRi5LPSFDc8vbh8/47btWKHbVsxf/lnmUtiE9l6rsyraeWl+guNfVV7COEMoHuM8exaxuQDDwGjYowf1Ha83fcYGGc8/cIWjjLzvvwqO2+5MX955m9ns6UN37nj5gdJDez4f1Q7B9Dk3fWjgZkOQc3YXkMHMXfunIw3v26T3zv2/ulfMh0GL190wNwY46DGft9MtAHMA2r99imbUZ0HDG+UiCRJkpRImShWZwCtQwgnb1wRQhgcQtg3hNCmbDkP2At4s4ZjSJIkNQ8JuLgqkxdYNXrDXowxhhCOAK4NIfwWWEv61lUPAX8OIUTSd1H5Y4zxtcaOT5IkScmRkatLyn7Nf3Q1m25p7FgkSZKUXJm/FFqSJEk1CjTvhxw0+8etSpIkKbksViVJkpRYtgFIkiQlXDPuAnBmVZIkScnlzKokSVLCeYGVJEmSlEAWq5IkSUos2wAkSZISrhl3ATizKkmSpOSyWJUkSVJi2QYgSZKUZMG7AUiSJEmJ5MyqJElSggW8wEqSJElKJItVSZIkJZZtAJIkSYkWvMBKkiRJSiKLVUmSJCWWbQCSJEkJ14y7AJp+sdoyBLZp3eTTqKJli+z8VA7fuWOmQ9ji8r57YaZDaBArZ1yc6RD0Ddx+3IBMhyBJDcI2AEmSJCVW9k1JSpIkZRnvBiBJkiQlkDOrkiRJSRaa9wVWzqxKkiQpsSxWJUmSlFi2AUiSJCVYwAusJEmSpESyWJUkSVJi2QYgSZKUcLYBSJIkSQnkzKokSVLCNeOJVWdWJUmSlFwWq5IkSUos2wAkSZISzgusJEmSpASyWJUkSVJi2QYgSZKUZMG7AUiSJEmJ5MyqJElSggWCF1g1d9Mfn0b/ot4U9enFVVdeXmV7aWkpxx87lqI+vRi+51CWLF5cvu2qKy6jqE8v+hf15onpjzdi1Jv3r+nTGLRbPwbs2ptr/nhFle2lpaX8eNwPGbBrbw7YZxhLliwGYO7sWew9dCB7Dx3IXkP3YMrkhxo58tpl4/k6cEgvXrnrdF7/5xn86ri9q2zvulM7Hr3mBGb9/Wc8ft14Cjq2Ld/22b8v5Plb/4fnb/0fJl32w8YMe7Oy8VxB9ub1xPRpDPh2X3brtwtXX1X9d8YJxx/Dbv12Yf/hw8rzmvGvJxg+bDBDB+7G8GGDeerfMxo58ppl67kyr6aVl+qn2RerqVSKs844lclTHuOlV+czacI9LJg/v9KY22+7lbzcPOa9sZDTzzyb88/7DQAL5s9n0sQJvPjKPB6eOo0zT/85qVQqE2lUkUql+NXZZ3DfQ1N54cXXuG/SRN5YUDmvO2+/jdzcPF56/U1+fvpZXPS7cwHoW7QrM599gWdemMv9Dz3C2Wf8jPXr12cijSqy8Xy1aBG49uwRHPbruxjwoxsZc8C36fOtjpXGXPbzg7j78ZcZ8uO/8L93PMUlp3yvfNuXpev4zkk38Z2TbmLMufc0dvg1ysZzBdmd1y/PPJ0HJj/C7Jdf5757J1T5zvhH2XfGK/Pf4tTTz+SC3/0WgB06dODe+yfzwtxX+Ovf/s7JJ52QiRSqyOZzZV5NJy/VX7MvVmfPmkXPnr3o3qMHrVq1YszYY5g6ZXKlMVOnTOa4cekv3x8ceRQzZzxJjJGpUyYzZuwxtG7dmm7du9OzZy9mz5qViTSqmDtnFj169qRb93ReRx51NI9OfbjSmEcfeZgfHj8OgMOOOJKnZs4gxsg222xDTk66Q2Rt6dpE/eohG8/X4L4FvFP8CYuXrWTd+hSTnnydkXv3qTSmT7eOzJy7CICnXlzEyL17ZyLUbyQbzxVkb15zZqe/MzbmdeSYsUydUvk745Epkzn2+B8BcPgPjmLmv9PfGbvtPoDO+fkA9O1XxNq1ayktLW30HDaVrefKvJpWXltKCJl/ZUqzL1ZLSoopLOxSvlxQUEhxcXHVMV3SY3Jycmjbrh0rVqyguLjqviUllffNlGUlJRQUfB1bfkEhy0pKahyTk5ND27bt+GTFCgDmzHqB7wzsz16Dd+dP1/2/8uI107LxfOV3aMvSD1eXLxd/tJqCjttXGvPawuUcvm8/AA7bpy9tt92a9m3bALB1qxyeufkUnvrLTxi1SZGbSdl4riB781pWUkxBpdgKWFayaV4l5fHn5OTQrm06r4omP3g/u+02gNatWzd80JuRrefKvJpWXqq/jFQgIYROwLXAYKAUWAycFWN8K4TQFlgAPBhjPK2hY4kxVhdf3cbUYd9MqS7mTX8sqi33QUOG8vzcV3nzjQX87OQfc+BBB7P11ls3SKzfRDaer+pC2DTUc//fdK45+1COP3h3nn11CcUfrmZ9agMAu4y5hmUrPqVb5zymXXsCr7/7AYtKVjZC5LXLxnMF5lXbmAXz53HB+efy0NRpWz7A/4Lnqpox5qUmqNFnVkP60/MgMDPG2DPG2A84D9ipbMilwFONFU9BQSFLl75fvlxcvJT8sl9nVRrzfnrM+vXrWbN6Ne3bt6egsOq+nTtX3jdT8gsKKC7+OraS4qV07ty5xjHr169nzZrV5LVvX2lM7z592WbbbVkw7/WGD7oOsvF8FX+0hsId25UvF3RsR8nHn1Yas2zFpxzzu4kM+8lNXHjLkwCs+by0fBvA4mUr+b+XF7P7zpXPc6Zk47mC7M0rv6CQ4kqxFdOp86Z5FZTHv379elavSecFULx0KT88+kj+euvt9OjZs/ECr0W2nivzalp5bSktQsj4K2O5Z+A99wfWxRhv2rgixvhyjPHpEMJA0kXr9MYKZtDgwSxc+DaLFy3iq6++YtLECYwYObrSmBEjR3P3nXcA8MD997Hv/t8lhMCIkaOZNHECpaWlLF60iIUL32bwkCGNFXqt9hg4mHcWLmTx4nRe9993L4eMGFVpzCGHjuKeu+4E0r+622ff/QkhsHjxovILqt57bwkL33qLrt/q1tgpVCsbz9ecN0roVdieb3XOZauclow5YFceefaNSmN2aLdN+SzBr48bzh2PvgRA7nZb02qrluVjhn27KwsWf9S4CdQgG88VZG9eAweVfWeU5XX/pImMGFn5O+PQkaP5513/AOChB+5j3/3S3xmrVq3iqCNGcfGlf2DYnntlIvxqZeu5Mq+mlZfqLxNtALsCczddGUJoAVwNjAMOqO0AIYRTgFMAunTtWq9gcnJyuOa6Gxg14iBSqRQnjD+RfkVFXHLRBewxcBAjR41m/IknceL4cRT16UVeXnvuvHsCAP2KijhyzNEM6N+PnJwcrr3+Rlq2bFmveLaUnJwcrvrTdRw5+lBSqRTH/2g8ffsV8YdLLmTAHoM4dOQoxo0/kZ+edAIDdu1NXl4et/3jnwA8/9yzXHv1leTkbEWLFi3447U3sEOHDhnOKC0bz1cqtYGzr32UKX8cR8sWLbjj0ZdYsPgj/r8T9+fFN0t45Nk32Wf3blzy0+8RY+SZV5Zw1jWPAOkLr/78q1Fs2BBp0SLwx7uf4Y0lyShWs/FcQXbn9cdrr+fwUYewIZVi3Ak/pm+/In5/8YUMGDiQESNH86PxJ3LyiT9it367kNe+PX8v+864+S838u47C7nisj9wxWV/AGDy1Gl03HHHTKaU1efKvJpOXltKc+5qCNX2NjbkG4ZwBtA9xnj2JutPA7aJMV4ZQhgPDKpLz+rAgYPisy/MaZhgM6h0XXbecqP1Vtn15QGQ990LMx1Cg1g54+JMh6BvYGMPc7bJadnsrwNWBu01dBBz587JeJnYtmvf+J3f/D3TYfDEacPmxhgHNfb7ZmJmdR5wVDXrhwHDQwg/B7YDWoUQPosx/rZRo5MkSVJiZOJH1hlA6xDCyRtXhBAGAzfHGLvGGLsBvwL+YaEqSZKau/R9TkPGX5nS6MVqTPcdHAEcGEJ4J4QwD7gIKKl1R0mSJDU7GbnPaoyxBDi6lu23A7c3VjySJElKpmQ8lkiSJEk1apHxy7wyx8ssJUmSlFjOrEqSJCVcc358rDOrkiRJ2iJCCAeHEN4MISwMIVR7V6cQwtEhhPkhhHkhhH9u7pjOrEqSJKneQggtgRuBA4GlwOwQwsMxxvkVxuwMnAvsFWNcGULY7KPuLFYlSZISrol0AQwBFsYY3wUIIUwADgPmVxhzMnBjjHElQIzxw80d1DYASZIk1UWHEMKcCq9TNtleALxfYXlp2bqKdgF2CSE8G0J4PoRw8Obe1JlVSZIk1cXHMcZBtWyvbv43brKcA+wM7AcUAk+HEHaNMa6q6aAWq5IkSQkWgFBtHZg4S4EuFZYLqfqE0qXA8zHGdcCiEMKbpIvX2TUd1DYASZIkbQmzgZ1DCN1DCK2AY4CHNxnzELA/QAihA+m2gHdrO6jFqiRJkuotxrgeOA14HFgA3BtjnBdCuCSEMLps2OPAihDCfODfwK9jjCtqO65tAJIkSQnXVB63GmN8FHh0k3UXVPhzBH5R9qoTZ1YlSZKUWM6sSpIkJVkIPm5VkiRJSiKLVUmSJCWWbQCSJEkJ14y7AJxZlSRJUnJZrEqSJCmxbAOQJElKsAC0aMZ9ABarCdV6q5aZDkF1tHLGxZkOoUHkDT4t0yE0iJWzb8h0CA0ip6W/KJOUnSxWJUmSEq4ZT6zasypJkqTksliVJElSYtkGIEmSlHA+blWSJElKIItVSZIkJZZtAJIkSQkWgncDkCRJkhLJmVVJkqSEa85PsHJmVZIkSYllsSpJkqTEsg1AkiQp4ZpvE4Azq5IkSUowi1VJkiQllm0AkiRJCefjViVJkqQEcmZVkiQpwQLQovlOrDqzKkmSpOSyWJUkSVJi2QYgSZKUZCF4gVVzN/3xafQv6k1Rn15cdeXlVbaXlpZy/LFjKerTi+F7DmXJ4sXl26664jKK+vSif1Fvnpj+eCNGvXnm1XTyysacAG668DiWPHkZcyadV+OYq885itcnX8isieeye5/C8vXHjRrKa5Mv4LXJF3DcqKGNEW6dZev5ysa8sjEnMK+mlpfqp9kXq6lUirPOOJXJUx7jpVfnM2nCPSyYP7/SmNtvu5W83DzmvbGQ0888m/PP+w0AC+bPZ9LECbz4yjwenjqNM0//OalUKhNpVGFeTSevbMxpozunPM9hp95Y4/aD9u5Hz64d2fWwiznt9/dw/XnHAJDXdhvOP+UQ9hn3R4YffxXnn3IIudu3aaywa5Wt5ysb88rGnMC8mlpeqr9mX6zOnjWLnj170b1HD1q1asWYsccwdcrkSmOmTpnMceNOAOAHRx7FzBlPEmNk6pTJjBl7DK1bt6Zb9+707NmL2bNmZSKNKsyr6eSVjTlt9OyL7/DJ6i9q3D5y3/78c2o63lmvLabd9m3o1KEtB+7Zlyeff4OVa75g1adf8uTzb/D9vfo1Vti1ytbzlY15ZWNOYF5NLa8tJYTMvzKl2RerJSXFFBZ2KV8uKCikuLi46pgu6TE5OTm0bdeOFStWUFxcdd+Sksr7Zop5NZ28sjGnusrfMZely1eWLxd/sIr8HXPJ75jL0g8qrP9wFfkdczMRYhXZer6yMa9szAnMq6nlpfrLSLEaQugUQpgQQngnhDA/hPBoCGGXEEIqhPBy2evhxoglxlhdfHUbU4d9M8W8qhmT0LyyMae6qi7UGGP166maayZk6/nKxryyMScwr2rHJDgv1V+jF6sh/el5EJgZY+wZY+wHnAfsBHwZY9y97DW6MeIpKChk6dL3y5eLi5eSn59fdcz76THr169nzerVtG/fnoLCqvt27lx530wxr6aTVzbmVFfFH6yisFNe+XLBTrks+2g1xR+uonCnCut3TK9Pgmw9X9mYVzbmBObV1PLaUkLZHQEy+cqUTMys7g+sizHetHFFjPHlGOPTGYiFQYMHs3Dh2yxetIivvvqKSRMnMGJk5Tp5xMjR3H3nHQA8cP997Lv/dwkhMGLkaCZNnEBpaSmLFy1i4cK3GTxkSCbSqMK8mk5e2ZhTXT3y1GscOzId75Bvd2PNZ1+y/OM1PPHcAr43rA+527chd/s2fG9YH554bkGGo03L1vOVjXllY05gXk0tL9VfJu6zuiswt4ZtW4cQ5gDrgctjjA9VNyiEcApwCkCXrl3rFUxOTg7XXHcDo0YcRCqV4oTxJ9KvqIhLLrqAPQYOYuSo0Yw/8SROHD+Ooj69yMtrz513TwCgX1ERR445mgH9+5GTk8O1199Iy5Yt6xXPlmJeTSevbMxpozsuG8/wgTvTIXc7Fk67lEtvepStctLx/e2+Z5j2zDwO2ruIeQ9fyBdr1/HTi+4CYOWaL7jslmk8c9c5APzvzdNYuabmC7UaU7aer2zMKxtzAvNqanltCc39cauhuv6PBn3DEM4AuscYz65mW36MsSSE0AOYARwQY3yntuMNHDgoPvvCnAaKVmq+8gaflukQGsTK2TdkOgRJTcReQwcxd+6cjJeJHXoUxZF/uCfTYXDHsbvNjTEOauz3zUQbwDxgYHUbYowlZf99F5gJDGi8sCRJkpQ0NRarIYS2tb3q8Z4zgNYhhJMrvNfgEMK+IYTWZcsdgL2A+TUcQ5IkqdnI9MVVmbzAqrae1XlAJN0qsdHG5Qj8V82iMcYYQjgCuDaE8FtgLbAYuBKYE0LYQLqIvjzGaLEqSZLUjNVYrMYYu9S0rb7Kft1/dDWbvt1Q7ylJkqSmp049qyGEY0II55X9uTCEUG3PqSRJkra8kIBXpmy2WA0h3ED63qjjylZ9AdxU8x6SJEnSllGX+6zuGWPcI4TwEkCM8ZMQQqsGjkuSJEmkH03dohk/PrYubQDrQggtSF9URQhhB2BDg0YlSZIkUbdi9UbgfqBjCOFi4BngigaNSpIkSaIObQAxxn+EEOYC3ytbNSbG+HrDhiVJkqSNmnEXQJ16VgFaAutItwJk4qlXkiRJaobqcjeA84F7gHygEPhnCOHchg5MkiRJqsvM6vHAwBjjFwAhhD8Ac4HLGjIwSZIkpWXycaeZVpdf6S+hclGbA7zbMOFIkiRJX6txZjWEcA3pHtUvgHkhhMfLlr9P+o4AkiRJagTNeGK11jaAjVf8zwMeqbD++YYLR5IkSfpajcVqjPHWxgxEkiRJ2tRmL7AKIfQE/gD0A7beuD7GuEsDxiVJkiQgEHzc6mbcDvwdCMAhwL3AhAaMSZIkSQLqVqxuE2N8HCDG+E6M8XfA/g0bliRJklS3+6yWhvTNvd4JIfwPUAzs2LBhSZIkCYDg3QA252xgO+AM0r2r7YATGzIoSZIkCepQrMYYXyj746fAuIYNR5IkSZtqzk+wqu2hAA+SfghAtWKMP2iQiCRJkqQytc2s3tBoUdTDhghr16UyHcYWl9Oi+f4E1dR8sLo00yE0iJWzm8RXwDd2/qNvZDqEBnHwzjtkOoQGMXznjpkOYYtbn9qQ6RAaRE7LulyzLX1ztT0U4MnGDESSJEnVa84/CjTn3CVJkpRwFquSJElKrLrcugqAEELrGGN2NudJkiQlVKB53w1gszOrIYQhIYTXgLfLlncLIfy5wSOTJElSs1eXmdXrgZHAQwAxxldCCD5uVZIkqZE055sE1aVntUWMcckm67LvXlGSJElKnLrMrL4fQhgCxBBCS+B04K2GDUuSJEmqW7H6M9KtAF2BD4B/la2TJElSI2jObQCbLVZjjB8CxzRCLJIkSVIlmy1WQwi3AHHT9THGUxokIkmSJKlMXdoA/lXhz1sDRwDvN0w4kiRJqiiE5n2f1bq0AUysuBxCuBN4osEikiRJksr8N49b7Q58a0sHIkmSJG2qLj2rK/m6Z7UF8Anw24YMSpIkSV/zbgA1COkGid2A4rJVG2KMVS62kiRJkhpCrcVqjDGGEB6MMQ5srIAkSZJUWTO+vqpOPauzQgh7NHgkkiRJ0iZqnFkNIeTEGNcDewMnhxDeAT4HAulJVwtYSZIkNajaZlZnlf33cKA3cCgwBjiq7L9Z41/TpzF4t37ssWtvrvnjFVW2l5aWcuK4H7LHrr353j7DeG/J4krb33//PQo7tuPP117dSBHXzRPTpzHg233Zrd8uXH1V9XmdcPwx7NZvF/YfPowlixcDMONfTzB82GCGDtyN4cMG89S/ZzRy5LXLxryemjGdA/fcje8O3ZWbrv9jle2z/vMMo783jN752/PYlAfL1xe//x6HHbgno747lIP3Gcg/77ilMcPerOmPT6N/UW+K+vTiqisvr7K9tLSU448dS1GfXgzfc2j5uQK46orLKOrTi/5FvXli+uONGPXmLXrxaf7+s0O49acHMeu+mv/O33r2cf50WF+Wv/06AKn165h27W+544zR3H7qCGbdd3NjhVwns5+ewYmHDmP8QUOYcMv1Vbbfd/tf+MnIvfnp4ftyzo+P5IPir2+5/berL+Hk0ftw8uh9mPnYQ40Zdq2y9TOYjd+DkL3nq74C0CKEjL8ypbZiNQDEGN+p7tVI8TW4VCrFr88+g0kPTeX5F1/j/kkTeWPB/Epj7rz9Ntrl5vHi62/ys9PP4qLfnVtp+/nn/JLvff/gxgx7s1KpFL8883QemPwIs19+nfvunVAlr3/cfhu5uXm8Mv8tTj39TC74XfomDzt06MC990/mhbmv8Ne//Z2TTzohEylUKxvzSqVSXPTbs7n1nw8x7ekXmfrgJN5+c0GlMfkFXbjyupsZ9YOxldZ33KkT9079N1NmvMD9jz3FX/98NR8sL2nM8GuUSqU464xTmTzlMV56dT6TJtzDgvmVz9Xtt91KXm4e895YyOlnns355/0GgAXz5zNp4gRefGUeD0+dxpmn/5xUKpWJNKrYkEox46+XcsSFNzP+him88fQjrHhvYZVxX33xOS9NvZNOu/QvX/fWs4+TWvcVJ1z/MMf96T5efXwiqz8orrJvJqRSKW74/W/4w1/v4ZYpzzDz0QdYsvDNSmN69f02N0yazl8feorhB43kb1dfAsALTz3B2/Nf5aYHZnD9hMeYdNuNfP7Zp5lIo5Js/Qxm4/cgZO/5Uv3VVqx2DCH8oqZXo0XYwObOmUWPnj3p1r0HrVq14gdHHc2jUx+uNOaxRx7mh8ePA+CwI47kqZkz2HhThEcensy3unenT99+jR57bebMTufVvUc6ryPHjGXqlMp5PTJlMsce/yMADv/BUcz8dzqv3XYfQOf8fAD69iti7dq1lJaWNnoO1cnGvF55cQ7f6t6Trt2606pVK0YcfhT/mja10pjCrt+iT9G3adGi8j/ZVq1a0bp1awC+Ki1lw4YNjRb35syeNYuePXuVn6sxY49h6pTJlcZMnTKZ48al/2f5gyOPYuaMJ4kxMnXKZMaMPYbWrVvTrXt3evbsxexZs6p7m0a3/O1Xye3UldxOXWi5VSv6DD+Ud2ZVnZ169p/XMfgHJ5HTqnX5uhAC60q/ZENqPetL19IiZytabbNtY4Zfozdfe5H8rt3p3KUbW7Vqxb6HHMFzM6ZVGrP70L3Zus02APTtP4iPPkj/YLRk4Zv0H7wnLXNyaLPNtvToXcScpzM/Y5etn8Fs/B6E7D1fqr/aitWWwHbA9jW8ssKykhIKCrqUL+cXFLKspPLMVEmFMTk5ObRt245PVqzg888/57o/XclvzrugUWOui2UlxRQUfp1XQUEBy0oqz+CUlJRQWPh1Xu3atmPFihWVxkx+8H52221AeUGUadmY1wfLS+icX1C+3Cm/4BvNjpYUL2XEfkMYvscunHLaL9ipU35DhPmNlZQUl58HgIKCQoqLNz1XxRR2qfBvq136XBUXV923pCQZM5CfrfiQ7Tt0Kl/eboed+HTFB5XGfPjufD79eDk9Bu9faf3Oe36frVq34a/j9+GWnxzAoMNPpM32uY0S9+Z8/MFyOnb6+nPYsVNnVny4rMbx0x64m8HDDwCgR58iZj/9JGu//ILVK1fwyqxn+Gh55s9Xtn4Gs/F7ELL3fG0pLRLwypTabl21LMZ4SUO8aQihE3AtMBgoBRYDZwFrgb8BXUg/iODQGOPihohho+puG1vl+bs1jLn89xfxs9PPYrvttmug6P57dclrc2MWzJ/HBeefy0NTp1UZlynZmFe18VL33qD8gkIemTmLD5aX8LMTxnLIyCPosONOWzLE/0q9zlVd/l1mTO2xxQ0bmHnr5Rx0xmVVxi1/+zVCi5ac8venKP1sDRPP/f/bu/P4qMrrj+OfIyMIVSC4AElQNiUkiOzUXXADWVwAQRShWGndtf21VWjVWq0L1q3aWvvTn4oLCC4IIqAo1pVVBUHUIKBJcEMBKxrMeH5/zBATkpAgSe6dm++7r3nVufe5c8/hmbk588xz7z2L/Q85lKYtWpVpW+t24n34/NNTef+dt7n5wcTc1B6H9+H95W9x6cgBNGm2Nx0P6UG9WKX3nKlxUX0PRvE4CNHtL9l1lc5ZrW7JGw08Ccx393bung2MB5oDDwIT3b0j0Av4rCZiKCk9I4P8EicJFOTn0aJlywrbFBUVsXnzJtKaNWPxooVcNeFyOme145933cEtE2/gnn/eVdMhV0l6Rib5eT/mlZ+fT4uWpUfcMjIyyMv7Ma9NmzfRrFmzRPu8PM44fQj/uvd+2rZrV3uBVyKKebVoWXpU5JOCfPZr0XIHW5SveYt0DszqyKIFr1VneD9ZRkZmcT8A5OfnkZ6+fV9lkvdxic/WpkRfZWSW3bZly3CMGO+5d3O+/uKT4uf/3fApezbbr/j51m+/4Yt1HzD1j2fzv+cey/r33mb6defzyQfvsOqlmbTudgT1YrvTqOnepHfsxqe57wSRRhn7tGhZajT080/W02y/FmXaLX3tJR695zb+fNeDOoVkuwAAIABJREFU1C8xxWHkry/j7idf5MZ7pwGQsX/bmg+6ElF9D0bxOAjR7a/qYhb8Iyg7KlaPraF99gG+d/e7ty1w97eADUDM3Z9LLvuvu2+poRiKdevek9W5uaxbu4atW7fyxLTH6D9gUKk2/U4axKMPTQISP5scdXQfzIxnn3+JZatWs2zVas674GJ+87vLGXfeBTUdcpV075HIa+2aRF6PT53CgIGl8zpp4GAeeehBAJ56YhpHH5PIa+PGjQw9dRB//st1HHrY4UGEX6Eo5tW5a3fWfZjLx+vWsnXrVp55ahrHnjigStuuL8jju2+/BWDTxq9YsvAN2rY7sCbDrbIePXuSm/tBcV9NnTKZAQMHl2ozYOBgHp70AABPPD6No/v0xcwYMHAwU6dMprCwkLVr1pCb+wE9e/UKIo0yWhx4MBvXr2PTp3nEv9/Kqpdn0bbXjz/3N/jZXpz/0Ov88t/z+OW/59GywyGcPOEftDiwE3vt25KPly3A3fn+uy2sf+9tmmUGX9QBdOjUlfx1H7I+bx3fb93KS88+yaF9TizVJnflcm7/8/9wzZ2TSNt73+Ll8XiczRu/BODD91bw4Xsr6X74MbUZfrmi+h6M4nEQottfsusq/J3G3b+soX12ApaUs/wgYKOZPQG0AZ4HLnf3Mqfzmdk4YBxAZqv9dymYWCzGTbfczpDBJxGPxznz7DF0zM7hr9dcRZduPThp4CBGjRnLr88ZTbdOHUhLS+PeBx/ZpX3Whlgsxs233cEpg/rzQzzOqNG/oGN2Dtf++Sq6du/OgIGDOXvMWM4dezaHZB9EWrNm/F8yr3v+eRcfrs7lxuuv48brrwNg+szZ7LvffjvaZa2IYl6xWIyrrr+FX4wYTDweZ9gZZ3NQVja33XgNnQ7pxnH9BrLszcWc94sRbN64kRfmzuL2idcy+z9LWP3Be1x/1RWYGe7OL8+7hA7ZnQLNZ5tYLMatt9/JoAEnEo/HGT1mLNk5OVxz9ZV0696DgYMGM2bsOYwdM4qcrPakpTVj0sOTAcjOyWHIsNPp2jmbWCzGbXfcRb169QLOKGG3ejH6jPsjj1/9S/yHH+h07Gnss/+BvPrwHbRo34l2vftWuG2Xk0Yy544JPHjRINwh59hT2bd1h1qMvmL1YjEunHAD488dzg8/xDnx1JG0PjCLB/5+AwfldOHQvv34981X8+2Wb/jLZecAsF96JtfcNYl40ff85qxEUdFoz724/Ma7QjENIKrvwSgeByG6/SW7zsqb/1GjOzS7GGjj7pdtt3wocC/QFfgImALMcvd7d/R6Xbv18BdfXVBT4QYmtpvm2qSKTzeF40za6pbRrGHQIdSICbNWBR1Cjeh34N5Bh1Ajjjxw38obpZiieHiu2lGdYvWCPAWnZhzeuwdLliwO/A9y+kEH+zl3PBF0GFzb/6Al7t6jtvcbxDtrBdC9nOV5wJvu/mHyzllPAbpLloiIiEgdFkSx+gLQwMzO3bbAzHoCDYA0M9v2NbovsLKc7UVERESkjqj1YtUT8w5OBY43s9VmtgK4GigA/geYZ2bLSVyNIFz3jhQREREJQNBXAgjyagCBzIB39wLg9HJWfQB0Lme5iIiIiNRBwZ+uKSIiIiI7VJfPu47eqXsiIiIiEhkqVkVEREQktDQNQERERCTEDNgtyDOcAqaRVREREREJLRWrIiIiIhJamgYgIiIiEnJ1eBaARlZFREREJLw0sioiIiISZqbrrIqIiIiIhJKKVREREREJLU0DEBEREQk5o+7OA9DIqoiIiIiElopVEREREQktTQMQERERCbHE7VaDjiI4GlkVERERkdBSsSoiIiIioaVpACIiIiIhp2kAIiIiIiIhpJFVERERkZAzq7tDqxpZFREREZHQSvmR1d0M9ti9XtBhVLtNW74POoQaUS+Ck26aNNo96BBkJ1zRt33QIdSIPjfODzqEGvHy+D5Bh1Dtovg3S6QmpXyxKiIiIhJlus6qiIiIiEhIqVgVERERkdDSNAARERGRMDOowxcD0MiqiIiIiISXRlZFREREQm63Ojy0qpFVEREREQktFasiIiIiElqaBiAiIiISYrrOqoiIiIhISKlYFREREZHQUrEqIiIiEnJmwT+qFqf1M7P3zCzXzC7fQbuhZuZm1qOy11SxKiIiIiK7zMzqAXcB/YFs4Awzyy6n3V7AxcCCqryuilURERGRUDN2C8GjCnoBue7+obtvBSYDJ5fT7i/ATcB3VXlRFasiIiIiUhX7mNniEo9x263PAD4u8TwvuayYmXUFWrn7zKruVJeuEhEREZGq+MLddzTHtLzhVy9eabYbcCswZmd2qmJVREREJMSMqp/gFLA8oFWJ55lAQYnnewGdgPmWSKgF8LSZDXb3xRW9qKYBiIiIiEh1WAQcaGZtzKw+MAJ4ettKd9/k7vu4e2t3bw28AeywUAUVqyIiIiJSDdy9CLgQmAO8Czzm7ivM7BozG/xTX1fFKjB3zmw653QgJ6s9E2+6ocz6wsJCzho5nJys9hx5WG/WrV1bvG7ijdeTk9WezjkdeG7unFqMunIvPD+Hw7vn8PMuHfn7LTeVWV9YWMi4MSP5eZeO9O97OB+tWwvA4489wrFH9Ch+tGzagHeWvVXL0Vds3nNz+HnXHHoeksXtfys/r1+OHknPQ7I4sc9hxXkBrHhnGf37HsERPQ/hqN5d+O67Kp2IWOOimBNE97MV1f46vP3ePH3JoTxz6WGcc+QBZdb/vv9BTD2/N1PP782MSw7l1fFHF68b3KUlMy89jJmXHsbgLi1rM+wden7ubHoekk23Th249eYby6wvLCxk7Kgz6NapA8cddWipvgL4+OOPyNy3CX+/7W+1FHHVRPWzFdW8dpklbrca9KMq3H2Wux/k7u3c/brksivd/ely2h5T2agqqFglHo9z6cUXMH3Gs7y5bCVTJz/KuytXlmpz/333ktY0jRWrcrnoksuYMP4PALy7ciVTp0xm6dsreHrmbC656Hzi8XgQaZQRj8e54reX8Mi0Gfxn4ds8+fgU3ltVOq9HHvw/mjZN44233uVX51/MtVeNB2DI6SOZ98pi5r2ymDv/9X+02r81nTp3CSKNMuLxOJf/9mImPzGDVxct48lpk8vk9fCD99G0aVMWvb2KX19wCddcmcirqKiI8385mom338Uri97mqVnz2H333YNIo5Qo5gTR/mxFsb92M5gwqAPnP/gWJ//9dfp3bkHbfX9Wqs1Nz77PsH8sYNg/FvDoG3nMW/k5AI0bxjivTxtG/mshI+9eyHl92tB4j+BPiYjH4/zusouZ+tRM3li6nMenTmHVu6X7atL999GkaRpL33mP8y66lKv/eEWp9RN+/1uOO6FfbYZdqSh/tqKYl+y6Ol+sLlq4kHbt2tOmbVvq16/PsOEjmDljeqk2M2dM58xRowE4bchQ5r8wD3dn5ozpDBs+ggYNGtC6TRvatWvPooULg0ijjDeXLKJN23Yc0CaR1ymnnc6cZ2aUajNn1gxOHzkKgIGnDOGVl17E3Uu1eXLaFE4denqtxV2ZpYsX0rptO1pvy2vIcJ6dWTqvZ5+ZwfBkXoNOGcLL81/A3Xlx3nNkdzqYTgcfAkCzvfemXr16tZ7D9qKYE0T3sxXV/jo4swkfbfiWvK++pSjuPLv8U/p03LfC9v07N+fZ5Z8AiRHZ11d/yeZvi9j8XRGvr/6Sww/cu7ZCr9CSxQtp2+7Hvjpt6OnMmll6cOfZZ57mjLMSfXXyqUN4KdlXAM88PZ0D2rQhq2OZa5oHKqqfrajmVV12Mwv8EVjuge05JAoK8snM/PHEtYyMTPLz88u2aZVoE4vFaNykCRs2bCA/v+y2BQWltw3K+oJ80jMyi5+3zMhg/fqC0m3W/9gmFouxV+MmfPnlhlJtpj8xjVOGDq/5gKto/foCMkrklZ6Rwfr1pf/NPykoICOzdH99uWEDq3Pfx8wYdspJ9D2iJ3+/9eZajb0iUcwJIvzZimh/7de4AZ9s+nFKwqebvqP5Xg3KbduyyR5kpDVkwYdflti28MdtNxeyX+Pyt61N6wsKyMj48X2UnpHJ+oLSx8GCEm1isRiNGyf66ptvvuH2W27iD+OvrNWYqyKqn62o5iW7LpDfacysBXAb0BMoBNYC04HzSjTLAka4+1M1Gcv2I4nJ+KrWpgrbBmWX8kpaunghDRs1pGN2p+oP8CfalbziRXEWvP4ac+e/TsNGjRgy8AQO6dqNo47pW2PxVkUUcwJ9tsprE+b+2uHFEbfTv3NznlvxGT/4tm3Lbl3OP0Gtq0pfVfReu+HaqznvokvZc889ayi6n06frXLahDgv2XW1PrJqiXfPk8D85OTbbGA8sNLdu7h7F6AvsAWYW9PxZGRkkpf3480W8vPzSE9PL9vm40SboqIiNm/aRLNmzcjILLtty5altw1KekYmBfl5xc/X5+fTokXpkx7S039sU1RUxNebN5GW1qx4/VOPP8apQ8IzqgqQnp5Bfom8CvLzadGi9L95y4wM8vNK91das2akZ2Rw6OFHsvc++9CoUSOOO7E/y956s1bjL08Uc4IIf7Yi2l+fbi6kRZM9ip83b7IHn31dWG7bfge3YNayT0ps+x0tmvw4ktq8cQM+r2Db2pSekUF+/o/vo4L8PFq0bFlhm6KiIjZvTvTV4kULuWrC5XTOasc/77qDWybewD3/vKtW469IVD9bUc2rOmy7zmrQj6AEMQ2gD/C9u9+9bYG7v+XuL5doMxR41t231HQwPXr2JDf3A9auWcPWrVuZOmUyAwaWvrrCgIGDeXjSAwA88fg0ju7TFzNjwMDBTJ0ymcLCQtauWUNu7gf07NWrpkOuki7devDh6lzWrU3k9dQTj3HCSQNLtTnhpIE89sgkAGY+9TiHH3VM8TfRH374gRlPPc4pQ8IzXxWga/eerCmZ1+NT6DegdF79ThrIlGReM556nCOO7oOZ0efYE1i5YjlbtmyhqKiI1175DwdldQwijVKimBNE97MV1f56J38zB+zdkIymexCrZ/Q/uDnzV31epl3rfRrReI8Yb3+8qXjZq7kbOLT93jTeI0bjPWIc2n5vXs3dUGbb2tate09W5/7YV09Me4z+AwaVatPvpEE8+lCir6Y/+ThHJfvq2edfYtmq1SxbtZrzLriY3/zucsadd0EQaZQR1c9WVPOSXRfENIBOwJJK2owAbqloZfJetOMAWu2//y4FE4vFuPX2Oxk04ETi8Tijx4wlOyeHa66+km7dezBw0GDGjD2HsWNGkZPVnrS0Zkx6eDIA2Tk5DBl2Ol07ZxOLxbjtjrtCc7JELBbjrzffxhmnDSAe/4EzzhpNVsccbrzuarp07c6JJw1i5KhfcOG4Mfy8S0eapqXxr/seKt7+9VdfpmV6Bge0aRtgFmXFYjGuv/l2Tj9lAD/8EOeMUWPI6pjDDdcm8uo3YBBnnj2W888dQ89DskhLS+Oe/3sYgKZpaZx34aWccPShmBnHndCPE/qdFGxCRDMniPZnK4r9Ff/B+evM97h7dFfq7WY8ubSA1Z99wwV927KiYDPzV30BQP+DWzB7+aeltt38bRH/mr+GR3+dKA7+9eKHbP62qNZz2F4sFuOmW25nyOCTiMfjnHn2GDpm5/DXa66iS7cenDRwEKPGjOXX54ymW6cOpKWlce+DjwQddqWi/NmKYl6y66y8+R81ukOzi4E27n5ZBetbAsuAdHf/vrLX6969h7+6oNJLdKWcTVsqTT0l1avqhdokcHuG4NJDNeG/3wVfRNWEPjfODzqEGvHy+D5Bh1Dt9thdRVSqOLx3D5YsWRz4H67WHTv7nx6YGXQY/LL3AUvcvUdt7zeIaQArgO47WH868GRVClURERERibYgitUXgAZmdu62BWbW08y23QrlDODRAOISERERCaWgT66qUydYeWLewanA8Wa22sxWAFcDBWbWGmgFvFTbcYmIiIhI+AQyKc3dC0j83F+ejNqMRURERETCK5pnUIiIiIhEhFG3bzlal3MXERERkZBTsSoiIiIioaVpACIiIiJhZhTfYbIu0siqiIiIiISWilURERERCS1NAxAREREJubo7CUAjqyIiIiISYhpZFREREQkxA3bTCVYiIiIiIuGjYlVEREREQkvTAERERERCru5OAtDIqoiIiIiEmIpVEREREQktTQMQERERCbk6fDEAjayKiIiISHhpZFVEREQk1Ayrw0OrGlkVERERkdBSsSoiIiIioaVpACIiIiIhZtTt0UUVqyHVsH69oEOoEfVjdfnjJmGw5x7RPOy9OqFv0CHUiOb9rgs6hGr31fN/CjoEkZSiykFEREREQiuaQwwiIiIiEaKrAYiIiIiIhJBGVkVERERCru6Oq2pkVURERERCTMWqiIiIiISWpgGIiIiIhJnpBCsRERERkVBSsSoiIiIioaVpACIiIiIhVtdvt1qXcxcRERGRkNPIqoiIiEjI6QQrEREREZEQUrEqIiIiIqGlaQAiIiIiIVd3JwFoZFVEREREQkzFqoiIiIiElqYBiIiIiIRcHb4YgEZWRURERCS8NLIqIiIiEmKJO1jV3aFVjawCc+fMpnNOB3Ky2jPxphvKrC8sLOSskcPJyWrPkYf1Zt3atcXrJt54PTlZ7emc04Hn5s6pxagr9/zc2XTv3JEuOQdxy8Qby6wvLCxkzFkj6JJzEH2PPJR169YCsGTRQo7o3Y0jenfj8F5dmTH9yVqOfMei2F9RzAmUV6rlFcVjxvG92vH2g+fzzsMX8D8jDyuzfv/mTZj1t7NYeO845tw2iox99yq1fq9G9Vk99RJuvaRfbYVcJVF9D0Y1L9k1db5YjcfjXHrxBUyf8SxvLlvJ1MmP8u7KlaXa3H/fvaQ1TWPFqlwuuuQyJoz/AwDvrlzJ1CmTWfr2Cp6eOZtLLjqfeDweRBplxONxfnvpRUyb/gwL33yHx6dOZtW7pfN68P77aJqWxlsr3uf8iy7hqgmXA9AxpxPzX13IKwuW8vj0WVx60XkUFRUFkUYZUeyvKOYEyisV84raMWO33YzbLunHyX94hK6j/8mwvp3IOmCfUm2uP+84Hp67jF7n3MNfH3iZa87tW2r9VWOP4eW3P6rFqCsX5fdgFPOSXVfni9VFCxfSrl172rRtS/369Rk2fAQzZ0wv1WbmjOmcOWo0AKcNGcr8F+bh7sycMZ1hw0fQoEEDWrdpQ7t27Vm0cGEQaZSxZNFC2rZrR5s2ibxOGzacZ2Y+XarNrJnTGXnm2QCcctpQXpr/Au5Oo0aNiMUSM0S+K/wuVLd4i2J/RTEnUF6pllcUjxk9s9JZnf8Va9dv5PuiH5j6wgoGHt6hVJusA/Zl/tI1ALz05tpS67se1IL9mu3J84tX12rclYnqezCqeVUXs+AfQanzxWpBQT6Zma2Kn2dkZJKfn1+2TatEm1gsRuMmTdiwYQP5+WW3LSgovW1QCgryySgVWwbrt8trfUFBcZtYLEbjxk34csMGABYvXEDvbgdzWI9DuPWOfxT/IQpaFPsrijmB8krFvKJ2zEjftzF5n28ufp7/+eYyP/MvX/0ppxzVEYCTj8yi8c8a0KxxQ8zghvOPZ/w/n6/VmKsiyu/BKOYluy6QYtXMWpjZZDNbbWYrzWyWmR1kZjeZ2Qoze9fM7rBa+Hru7uXFV7U2Vdg2KLuUF9CjV28WLF3Oi68s4JaJN/Ldd9/VTKA7KYr9FcWcQHmV2yaqeRHOY0Z5/7Lb53DFP5/jyEMO4PV/n8uRh+xP/uebKYr/wK9O6cGcN3JLFbthofdgOW1CnJfsulovVpMF6JPAfHdv5+7ZwHigJXA40BnoBPQEjq7peDIyMsnL+7j4eX5+Hunp6WXbfJxoU1RUxOZNm2jWrBkZmWW3bdmy9LZBycjIJL9UbPm02C6v9IyM4jZFRUVs3ryJtGbNSrXpkNWRn/3sZ6xc8U7NB10FUeyvKOYEyisV84raMSP/881k7tu4+HnGvo0p+OK/pdqs3/BfRlw5lUPP/TdX3fsiAJu/KaR3dia/PrUnqyZfxPXnHc/IEzrzl3Gl57MGJcrvwSjmVT0sFP8LShAjq32A79397m0L3P0tYCuwB1AfaADsDnxa08H06NmT3NwPWLtmDVu3bmXqlMkMGDi4VJsBAwfz8KQHAHji8Wkc3acvZsaAgYOZOmUyhYWFrF2zhtzcD+jZq1dNh1wl3Xr0ZHVuLmvXJvJ6YuoUThowqFSbkwYM5pGHHwTgqSemcdTRfTAz1q5dU3xyxEfr1vHB++9xwAGtazuFckWxv6KYEyivVMsriseMxe8V0D6zGQe0aMrusd0Y1jeHZ157v1SbvZs0LJ6L97uRR/DArLcA+MV1T3HQ8DvIGvF3rvjnczwydxl/uueF2k6hXFF9D0Y1L9l1QUwq6gQs2X6hu79uZi8C60n8enOnu79b3guY2ThgHECr/fffpWBisRi33n4ngwacSDweZ/SYsWTn5HDN1VfSrXsPBg4azJix5zB2zChystqTltaMSQ9PBiA7J4chw06na+dsYrEYt91xF/Xq1duleKpLLBbj5lvv4LRB/YnH45w1+hd0zM7humuuomu37pw0cDCjxoxl3Niz6ZJzEGlpzbhv0iMAvPHaK9x6803svvvu2G678bfb72TvffapZI+1I4r9FcWcQHmlYl5RO2bE485lt89mxsSR1NvNeODZt3l37ef86RdHs/S99Tzz2vsc1aU115zbB3d4ZdlHXHrbs0GHXakovwejmJfsOitv/keN7tDsYqCNu1+23fL2wO3A8OSi54A/uPt/dvR63bv38FcXLK6RWIO0teiHoEOoEfVjdf6cPpEaEdVjRvN+1wUdQrX76vk/BR2CVNHhvXuwZMniwCe/HpjTxW9/bG7QYTCgU/Ml7t6jtvcbROWwAuhezvJTgTfc/b/u/l/gWeDntRqZiIiIiIRKEMXqC0ADMzt32wIz6wk0Ao42s5iZ7U7i5KpypwGIiIiI1BXbbrca9CMotT5n1d3dzE4FbjOzy4HvgLXAb4B0YDngwGx3n1Hb8YmIiIhIeARy1WZ3LwBOL2fVr2o7FhEREREJr+BvMSIiIiIiFQv4dqdB06nZIiIiIhJaKlZFREREJLQ0DUBEREQk5DQNQEREREQkhDSyKiIiIhJyFuB1ToOmkVURERERCS0VqyIiIiISWpoGICIiIhJiBuxWd2cBaGRVRERERMJLxaqIiIiIhJamAYiIiIiEnK4GICIiIiISQhpZFREREQk53cFKRERERCSEVKyKiIiISGhpGoCIiIhIyOkEKxERERGREFKxKiIiIiKhpWkAIiIiIiFW12+3qmI1pOrHNOgtIlUX1WPGV8//KegQql1azwuDDqFGfLXozqBDkIhSsSoiIiISaqYTrEREREREwkjFqoiIiIiElqYBiIiIiISZ6XarIiIiIiKhpGJVREREREJL0wBEREREQq4OzwLQyKqIiIiIhJeKVREREREJLU0DEBEREQmxxO1W6+5EAI2sioiIiEhoaWRVREREJOTq7riqRlZFREREJMRUrIqIiIhIaGkagIiIiEjY1eF5ABpZFREREZHQUrEqIiIiIqGlaQAiIiIiIWd1eB6ARlZFREREJLQ0sioiIiIScnX4BlYaWQWYO2c2nXM6kJPVnok33VBmfWFhIWeNHE5OVnuOPKw369auLV438cbryclqT+ecDjw3d04tRl055ZU6eUUxJ1Beyit4UcwJ4O6rzmTdvOtZPHV8hW3+9vuhvDP9KhZOuYIuWZnFy88c1Jvl069k+fQrOXNQ79oIt8qi2l+ya+p8sRqPx7n04guYPuNZ3ly2kqmTH+XdlStLtbn/vntJa5rGilW5XHTJZUwY/wcA3l25kqlTJrP07RU8PXM2l1x0PvF4PIg0ylBeqZNXFHMC5aW8gs8rijltM2nGG5x8wV0Vrj/xiGza7b8vnU7+Mxde+yh3jB8BQFrjRkwY15+jRt3MkWdNZMK4/jTdq2Fthb1DUe4v2TV1vlhdtHAh7dq1p03bttSvX59hw0cwc8b0Um1mzpjOmaNGA3DakKHMf2Ee7s7MGdMZNnwEDRo0oHWbNrRr155FCxcGkUYZyit18opiTqC8lFfweUUxp21eXbqaLzdtqXD9wKM788jMRLwLl6+lyV4NabFPY44/rCPz3ljFV5u3sPHrb5n3xipOODy7tsLeoSj3V3WwEDyCUueL1YKCfDIzWxU/z8jIJD8/v2ybVok2sViMxk2asGHDBvLzy25bUFB626Aor9TJK4o5gfJSXsHnFcWcqip9v6bkffJV8fP8TzeSvl9T0vdtSt6nJZZ/tpH0fZsGEWIZdbm/ZMcCKVbNrIWZTTaz1Wa20sxmmdlBZnajmb2TfAyvjVjcvbz4qtamCtsGRXmV0yakeUUxJ1Be5bZRXrUqijlVVXmhunv5yymbaxDqcn/JjtV6sWqJd8+TwHx3b+fu2cB44AygG9AF6A38zswa13Q8GRmZ5OV9XPw8Pz+P9PT0sm0+TrQpKipi86ZNNGvWjIzMstu2bFl626Aor9TJK4o5gfJSXsHnFcWcqir/041ktkgrfp7RvCnrP99E/mcbyWxeYvl+ieVhUJf7q0qCngMQYO0fxMhqH+B7d7972wJ3fwvYArzk7kXu/g3wNtCvpoPp0bMnubkfsHbNGrZu3crUKZMZMHBwqTYDBg7m4UkPAPDE49M4uk9fzIwBAwczdcpkCgsLWbtmDbm5H9CzV6+aDrlKlFfq5BXFnEB5Ka/g84piTlX1zEvLGTkwEW+vg1uz+b/f8skXm3nutXc57tAsmu7VkKZ7NeS4Q7N47rV3A442oS73l+xYENdZ7QQsKWf528BVZnYL0IhEUbuynHaY2ThgHECr/fffpWBisRi33n4ngwacSDweZ/SYsWTn5HDN1VfSrXsPBg4azJjyynAoAAAaFUlEQVSx5zB2zChystqTltaMSQ9PBiA7J4chw06na+dsYrEYt91xF/Xq1duleKqL8kqdvKKYEygv5RV8XlHMaZsHrh/Dkd0PZJ+me5I7+y/85e5Z7B5LxPe/015h9isrOPGIHFY8fRVbvvueX139EABfbd7C9f+ezSsP/R6Av94zm682V3yiVm2Kcn/tqsTAZt2d1mDlzf+o0R2aXQy0cffLylk3ARgGfA58Bix099t39Hrdu/fwVxcsrpFYRUREqltazwuDDqFGfLXozqBDqHaH9+7BkiWLA68Ssw/u6g8+/VLQYdCzbZMl7t6jtvcbxDSAFUD38la4+3Xu3sXdjyfxReKDWo1MREREREIliGL1BaCBmZ27bYGZ9TSzo81s7+TzzkBnYG4A8YmIiIiEhyWu8BD0Iyi1PmfV3d3MTgVuM7PLge+AtcDlwMvJS01sBs5y96Lajk9EREREwiOIE6xw9wLg9HJWheM2GiIiIiISCoEUqyIiIiJSdYGf5RWgOn+7VREREREJL42sioiIiIRdHR5a1ciqiIiIiISWilURERERCS1NAxAREREJNavTt1vVyKqIiIiIhJaKVREREREJLU0DEBEREQm5IG93GjSNrIqIiIhItTCzfmb2npnlmtnl5az/jZmtNLNlZjbPzA6o7DVVrIqIiIiEmIXkUWmcZvWAu4D+QDZwhpllb9fsTaCHu3cGpgE3Vfa6KlZFREREpDr0AnLd/UN33wpMBk4u2cDdX3T3LcmnbwCZlb2oilURERERqYp9zGxxice47dZnAB+XeJ6XXFaRc4BnK9upTrASERERCbtwnGD1hbv32MH68qL0chuanQX0AI6ubKcqVkVERESkOuQBrUo8zwQKtm9kZscBE4Cj3b2wshfVNAARERERqQ6LgAPNrI2Z1QdGAE+XbGBmXYF/AYPd/bOqvKhGVkVERERCLhVut+ruRWZ2ITAHqAfc5+4rzOwaYLG7Pw1MBPYEplri4rEfufvgHb2uilURERERqRbuPguYtd2yK0v893E7+5qaBiAiIiIioaWRVREREZGQq8u3W1WxKiIiUou+WnRn0CHUiLSeFwYdQrUrfO+joEMQVKyKiIiIhF4dHljVnFURERERCS8VqyIiIiISWpoGICIiIhJmRp2eB6CRVREREREJLRWrIiIiIhJamgYgIiIiEnKpcLvVmqKRVREREREJLY2sioiIiISYUbfvYKWRVREREREJLRWrIiIiIhJamgYgIiIiEnJ1eBaARlZFREREJLxUrIqIiIhIaGkagIiIiEjY1eF5ABpZFREREZHQ0siqiIiISMjpDlYiIiIiIiGkYlVEREREQkvFKjB3zmw653QgJ6s9E2+6ocz6wsJCzho5nJys9hx5WG/WrV1bvG7ijdeTk9WezjkdeG7unFqMunLKK3XyimJOoLyUV/CimBNEM6+7rzqTdfOuZ/HU8RW2+dvvh/LO9KtYOOUKumRlFi8/c1Bvlk+/kuXTr+TMQb1rI9xaZxb8Iyh1vliNx+NcevEFTJ/xLG8uW8nUyY/y7sqVpdrcf9+9pDVNY8WqXC665DImjP8DAO+uXMnUKZNZ+vYKnp45m0suOp94PB5EGmUor9TJK4o5gfJSXsHnFcWcILp5TZrxBidfcFeF6088Ipt2++9Lp5P/zIXXPsod40cAkNa4ERPG9eeoUTdz5FkTmTCuP033alhbYUstqPPF6qKFC2nXrj1t2ralfv36DBs+gpkzppdqM3PGdM4cNRqA04YMZf4L83B3Zs6YzrDhI2jQoAGt27ShXbv2LFq4MIg0ylBeqZNXFHMC5aW8gs8rijlBdPN6delqvty0pcL1A4/uzCMzE7EuXL6WJns1pMU+jTn+sI7Me2MVX23ewsavv2XeG6s44fDs2gpbakGdL1YLCvLJzGxV/DwjI5P8/PyybVol2sRiMRo3acKGDRvIzy+7bUFB6W2DorxSJ68o5gTKS3kFn1cUc4Lo5lWZ9P2akvfJV8XP8z/dSPp+TUnftyl5n5ZY/tlG0vdtGkSINcpC8AhKjRarZtbCzCab2WozW2lms8zsIDObbWYbzWzmdu3bmNkCM/vAzKaYWf2ajA/A3cuLu2ptqrBtUJRXOW1CmlcUcwLlVW4b5VWropgTRDevypQXpruXv5yyeUrqqrFi1RLv/ieB+e7ezt2zgfFAc2AiMKqczW4EbnX3A4GvgHNqKr5tMjIyycv7uPh5fn4e6enpZdt8nGhTVFTE5k2baNasGRmZZbdt2bL0tkFRXqmTVxRzAuWlvILPK4o5QXTzqkz+pxvJbJFW/DyjeVPWf76J/M82ktm8xPL9EssjJ+hh1YieYNUH+N7d7962wN3fcveX3X0e8HXJxsniti8wLbnoAeCUGowPgB49e5Kb+wFr16xh69atTJ0ymQEDB5dqM2DgYB6e9AAATzw+jaP79MXMGDBwMFOnTKawsJC1a9aQm/sBPXv1qumQq0R5pU5eUcwJlJfyCj6vKOYE0c2rMs+8tJyRAxOx9jq4NZv/+y2ffLGZ5157l+MOzaLpXg1puldDjjs0i+deezfgaKU61eQdrDoBS3ai/d7ARncvSj7PAzLKa2hm44BxAK32339XYiQWi3Hr7XcyaMCJxONxRo8ZS3ZODtdcfSXduvdg4KDBjBl7DmPHjCInqz1pac2Y9PBkALJzchgy7HS6ds4mFotx2x13Ua9evV2Kp7oor9TJK4o5gfJSXsHnFcWcILp5PXD9GI7sfiD7NN2T3Nl/4S93z2L3WCK2/532CrNfWcGJR+Sw4umr2PLd9/zq6ocA+GrzFq7/92xeeej3APz1ntl8tbniE7Uk9Vh581qq5YXNLgbauPtlFaw/Bvgfdx+YfL4v8Lq7t08+bwXMcveDd7Sf7t17+KsLFldr7CIiIrJz0npeGHQI1a7wvcf4YctngU/qPfiQbv7E3FeDDoODWjRa4u49anu/NTkNYAXQfSfafwE0NbNto72ZQEG1RyUiIiIiKaMmi9UXgAZmdu62BWbW08yOLq+xJ4Z4XwSGJheNBqaX11ZERERE6oYaK1aTxeepwPHJS1etAK4GCszsZWAqcKyZ5ZnZicnN/gD8xsxyScxhvbem4hMRERFJCdV4y9RUvN1qTZ5ghbsXAKeXs+rICtp/CKTGaYkiIiIiUuNqtFgVERERkV0X+FleAarzt1sVERERkfBSsSoiIiIioaVpACIiIiJhV4fnAWhkVURERERCS8WqiIiIiISWpgGIiIiIhJphdXgegEZWRURERCS0VKyKiIiISGhpGoCIiIhIyAV5u9OgaWRVREREREJLI6siIiIiIWbU6cusamRVRERERMJLxaqIiIiIhJamAYiIiIiEXR2eB6CRVREREREJLRWrIiIiIhJamgYgIiIiEnK63aqIiIiISAhpZFVEREQk5HQHKxERERGREEr5kdWlS5d80XB3W1dLu9sH+KKW9lVbopgTKK9UE8W8opgTKK9UE8W8ajOnA2ppP7IDKV+suvu+tbUvM1vs7j1qa3+1IYo5gfJKNVHMK4o5gfJKNVHMK4o5VUUdngWgaQAiIiIiEl4qVkVEREQktFJ+GkAtuyfoAGpAFHMC5ZVqophXFHMC5ZVqophXFHPaMavbVwMwdw86BhERERGpQOeu3X3WC68FHQatmu2xJIj5whpZFREREQm9uju0qjmrIiIiIhJaKlZFJFBmdXkmloiIVEbF6k4ws0j+e6lYSA1mVi/5/1Hrr/pBB1DdzGzvoGOoCWbWNqq5RVkEjxnFOUUxt/IYiROsgn4EJZLFV3UzswwAd/8hKgWrmXU1s5PMLNsjdJadmfUyszPNrHvQsVQnMzsOuNLM9nZ3j8oB2sz6Af8ys/oRyulkYIaZZW77ghEFZtYfWAr8PPk8Kv11lJn9xcxONrOfBR1PdTGzbsljfNugY6khzZL/X6eK1roqEoVXTTKzQcALZnYFRKNgTf7ReQwYAMw1sxOSy1P6w25mJwKTgEOA/5hZyv9RLRH7RcBJwG/MrEWyYE3pQijZX1cDD7v71ih8aTKzQ4HrgKvdPc/d40HHVB2SffVHYDrwRzPbLyL91R+4D3Dgr0Ak7oqU/BL4BHAE8C/gt2Z2ULBRVR8zOwl4wszuACaYWbPkMTGl/zZLxXQ1gB0ws/2BicBkoIOZXe7uN2wrWN39h4BD3Glm1gG4DTjH3f9jZguA28yst7t/HXB4P5mZdQLuBMa5+4tmtoVEn61z9/UBh/eTlSgI5gAdgI0kCtbxJP7ApiQzywKeBY5393lm1gJoDnwH5KZwkbcX8KC7z00eP/oD+UC+u78ZbGg/jZn1IPHZGuvuL5vZv0kUdbPMrF6q9pWZNQHGAOcn++troLeZbQbWu/sngQb4E5nZnsBoEsf4eWZ2LImCtbmZPeDu7wQb4a5J/g37B3AOiSlEx5AoXIe5++dmZlH4IlWelB11qQb6FlIBM+tK4g/PVcCtJL59dzOzyyExwhpgeD+JmXUj8dPJFclCdTd3fxBYTQq/F5J57Qv8Klmo7g/8HjgWmGZm5ycP4CnFzLonpzU0Al4H9gAWAluA+4H7zKxhqo2wJj9bceBVEoX3AcAU4LfAU8CvkzmnDDPrYWa9gBzgCDPbh0QfdQaGAueb2WEBhviTJAvV7sB57v5ycvEGYBxACheq3YADSHyRODP5ZXcCkA3cDFxoZu0DDPEnSebVlsTxvBOAu88jcfzYkx+ncKTs8R74GpidzGsuiX57A5hiZmlRLVTrulR+w9YYMxsAPAR0ARa7+0YSH/a/kyhYr0i2O8jM0oKLtOqSOT0AtAPehlIFd2OgRbJdGzNLmRNeknndD7QCPkouzgJ+7+5nkziQnZtcljKSed0HdCSR2+fAXu7+ElAAnEqieI2nUsGQzOthEp+twcBmYA0wJdlf5wNjSaH+KtFX7UgU2/nA/wKz3P0CEl94NwGtg4rxpzCzgSTuFFQIvF9iSsofgf3MbGxgwe2CZH89SOKzNQnYClwL3OXuY0h8acoi0Z8po0ReXYDFQBsz+4eZ3QrsDjwJXGxm+6TiYAsUF9k/AD83s9HuHnf3IuBKEjmPtaRAA60hQZ9cFeS/qqYBbMfMjgZuB8509wXblrv792a2iMTPYaPN7HkSP1v2CSbSqiuR01nu/kaJ5fWBIqAhsMXMRgC/Ak4mcQAPtYrycve5JL5x4+7zzWwhsF8wUe68it6DZvahmf0e+DWJP66NgfFmdm3ygB1qFeR1hpk94u4zAJIj40tJjJSHXnk5mdkkEgXdYOBmd19nZltJjNqRCj9TJvO6jbLHjHruXpTMsV1yWejz2aaC9+C5ZnYu0AbA3d8ys/Uk+mtOMJHunO2PhclfJroBh5MYlPp98m/YQlLw776ZZbh7frLI/sTMLgAeNLNv3f0x4HtgAXBYqrwXZeek3Ju2FnQH/u7uC8wsljwwmyd8R+LEnVNIfCvv7+5fBBtulWzL6Y3tctoKYGavA1cABwMXuPvmIIPdCRXlVXywMrMzSRyw/xpYlDuv1HuQH+emfk1i1PFX7v5c8mfl1alQqCZtnxfuXrStUIXi/joM+EtAMe6skjnt7u7fA0tInDh2vpk9DswHhgCDoNQ85DCr6LO1bRT/OeB1M5vv7s8FGOfOKtVfQFGyP14GspPTvD4DTiBxAmqqKNlfDdx9C/BK8gFAciS8Fyk2190SJznfbGb3u/v1AO7+arJg/aeZ7enu95lZUyDHEld02JIinzOpIhWrSSWKnDYkfrKDxLy64j8uybl2TUkcxPq5+/IgYq2qKuaUSeIbeA7wc3d/P4hYd0YV88oCegLjgSHuvi6IWHdGZXmZ2T1A3rbiwN2Dv1F0FVSU13ZfKjqR+On1GmCQu39U9pXCo4KcthV0hcBCS5zkNxD4L3BKhD5bXd39TTO7FAj95woq7q8S78GPSRSpHYGuwGnunlv7ke6cCvLaNgixmydOBj6ERF5XAye5+6eBBPsTWAUnOQO4+2wzGwncYmZHkLjywWnu/k1wEdcsq8OnWGnOalKJg9aTJObDdE8WCLvZj5PRjyExL/KIsBeqUOWchibXd0+FP6ZQ5bwGkjh58kR3XxlIoDuporxI/E3a3d2/IjFXsEtwUe68KvbX8STm4h6ZCv1VSV9tO+HtGBLzVv/P3d8NIs6dVcW+OtbMDnH3hyNyzIglC5zvSFx2bIy7rwgs2J2wo7xKNOtD4jyFXp5CVwKwSk5yBnD310kMHv2WxLEjZfKTnaNitawFJH46GZ784P+Q/HY6HBgFFLr758GGuNMqymkEcDrwuLuvCTbEn2RHeQ0F5od9hK4C5eX1ffI9OBL4MtjwfrId9deZwMfuXhBsiDutvJziyb4aQ+JSY6loR8fB4UTrPViUfA+OAr5192+DDfEn2VF/nQ187Sl0KS6r+knOWQDu/lUqjRjLzjNN6yjLEnesOofEpY8WkfjGPRQYmqrf3HaQ07BUGCWuSB3MK2XfgxDN/qqDfaW8QigqeVniZLF72e4E0+S6PUjMux1N4tJjzYE+nhrnjuySQ7p29zkvvVF5wxrWskn9Je5e6zfPULFaATNrSGLS+nHAeuDFVPnJqyJRzAmUV6qJYl5RzAmUV6qJQl5m9hsSc9pvt4pPnL2FxAh/f3dfFliwtUjFqopVERERCdC2gtTM/g5scvc/llOkbjvJ+W4SI8Yp+WvMT1HXi1XNWRUREZFARfEk5+pmIXgERcWqiIiIhEUUT3KWXaTrrIqIiEgouPs3ZvZvEieL3WKJO0eWPFksL9AAAxL07U6DpmJVREREQsPd881sIvACP54sNjjVThaT6qNiVUREREIleb3bUreMlbpLxaqIiIhIyOl2qyIiIiIiIaRiVURERERCS9MARERERMKu7s4C0MiqiIiIiISXilURqTFmFjezt8zsHTObamaNduG1jjGzmcn/Hmxml++gbVMzO/8n7ONqM/ufqi7frs39ZjZ0J/bV2sze2dkYRaRuCvruVbqDlYhE1bfu3sXdOwFbgV+XXGkJO30ccven3f2GHTRpCux0sSoiIuGjYlVEasvLQPvkiOK7ZvYPYCnQysxOMLPXzWxpcgR2TwAz62dmq8zsFeC0bS9kZmPM7M7kfzc3syfN7O3k4zDgBqBdclR3YrLd78xskZktM7M/l3itCWb2npk9D3SoLAkzOzf5Om+b2ePbjRYfZ2Yvm9n7ZjYw2b6emU0sse9f7eo/pIhIXaJiVURqnJnFgP7A8uSiDsCD7t4V+Ab4I3Ccu3cDFgO/MbM9gH8Dg4AjgRYVvPwdwEvufgjQDVgBXA6sTo7q/s7MTgAOBHoBXYDuZnaUmXUHRgBdSRTDPauQzhPu3jO5v3dJ3BZym9bA0cAA4O5kDucAm9y9Z/L1zzWzNlXYj4hIsW23XA3yERRdDUBEalJDM3sr+d8vA/cC6cA6d38jufznQDbwqiWOhvWB14EsYI27fwBgZg8B48rZR1/gbAB3jwObzCxtuzYnJB9vJp/vSaJ43Qt40t23JPfxdBVy6mRm15KYarAnMKfEusfc/QfgAzP7MJnDCUDnEvNZmyT3rVtHiohUgYpVEalJ37p7l5ILkgXpNyUXAc+5+xnbtesCeDXFYcD17v6v7fZx6U/Yx/3AKe7+tpmNAY4psW771/Lkvi9y95JFLWbWeif3KyJSJ2kagIgE7Q3gcDNrD2BmjczsIGAV0MbM2iXbnVHB9vOA85Lb1jOzxsDXJEZNt5kDjC0xFzbDzPYD/gOcamYNzWwvElMOKrMXsN7MdgfO3G7dMDPbLRlzW+C95L7PS7bHzA4ys59VYT8iIkkWiv8FRSOrIhIod/88OUL5qJk1SC7+o7u/b2bjgGfM7AvgFaBTOS9xCXCPmZ0DxIHz3P11M3s1eWmoZ5PzVjsCrydHdv8LnOXuS81sCvAWsI7EVIXK/AlYkGy/nNJF8XvAS0Bz4Nfu/p2Z/S+JuaxLLbHzz4FTqvavIyIi5l5dv7KJiIiISHXr2q2Hv/DKgqDDoNnPYkvcvUdt71fTAEREREQktFSsioiIiEhoqVgVERERkdBSsSoiIiIioaViVURERERCS5euEhEREQm5IG93GjSNrIqIiIhIaGlkVURERCTkgryDVNA0sioiIiIioaViVURERERCS9MARERERMLMdIKViIiIiEgoqVgVERERkdDSNAARERGRELPko67SyKqIiIiIhJZGVkVERETCrg4PrWpkVURERERCS8WqiIiIiISWpgGIiIiIhJxutyoiIiIiEkIqVkVEREQktDQNQERERCTkdLtVEREREZEQUrEqIiIiIqGlaQAiIiIiIVeHZwFoZFVEREREwksjqyIiIiJhV4eHVjWyKiIiIiKhpWJVREREREJL0wBEREREQk63WxURERERCSEVqyIiIiJSLcysn5m9Z2a5ZnZ5OesbmNmU5PoFZta6stdUsSoiIiISYkbidqtBPyqN06wecBfQH8gGzjCz7O2anQN85e7tgVuBGyt7XRWrIiIiIlIdegG57v6hu28FJgMnb9fmZOCB5H9PA44123EprBOsREREREJs6dIlcxrubvsEHQewh5ktLvH8Hne/p8TzDODjEs/zgN7bvUZxG3cvMrNNwN7AFxXtVMWqiIiISIi5e7+gY6ii8kZI/Se0KUXTAERERESkOuQBrUo8zwQKKmpjZjGgCfDljl5UxaqIiIiIVIdFwIFm1sbM6gMjgKe3a/M0MDr530OBF9x9hyOrmgYgIiIiIrssOQf1QmAOUA+4z91XmNk1wGJ3fxq4F5hkZrkkRlRHVPa6VkkxKyIiIiISGE0DEBEREZHQUrEqIiIiIqGlYlVEREREQkvFqoiIiIiElopVEREREQktFasiIiIiEloqVkVEREQktP4fIIqvN82Y11wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 10)\n",
    "cnf_matrix = confusion_matrix(y_validation_cls, y_validation_predict_cls)\n",
    "plot_confusion_matrix(cnf_matrix, [f'C{i+1}' for i in range(num_classes)], \n",
    "                      title='Confusion matrix', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToJSONFile(path, fileName, data):\n",
    "    filePathNameWExt = path + '/' + fileName + '.json'\n",
    "    with open(filePathNameWExt, 'a') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = classification_report(y_validation, y_validation_score, [0,1,2,3,4,5,6,7,8,9], digits=2, output_dict=True)\n",
    "writeToJSONFile(f'{data_path}{\"pack_detector\"}', \"tmp\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "matrix={}\n",
    "average_precision=[0]*10\n",
    "with open(f'{data_path}{\"pack_detector/\"}{\"tmp.json\"}', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "    for i in range(num_classes):\n",
    "        average_precision[i] = average_precision_score(y_validation[:, i], y_validation_score[:, i])\n",
    "        matrix[str(i)]={\"mAP\": average_precision[i].tolist(), \"precision\" : data[str(i)]['precision'], \"recall\" : data[str(i)]['recall']}\n",
    "    writeToJSONFile(f'{data_path}{\"pack_detector\"}', \"metrics\", matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Products Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CROP_TRIALS = 6\n",
    "CROP_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_between(s, f):\n",
    "    if s == f:\n",
    "        return s\n",
    "    return np.random.randint(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_products, eval_products = [], []\n",
    "for img_file, is_train in training_df[['file', 'is_train']].values:\n",
    "    if is_train:\n",
    "        img_path = shelf_images_train\n",
    "    else:\n",
    "        img_path = shelf_images_test\n",
    "    img = cv2.imread(f'{img_path}{img_file}')\n",
    "    img_h, img_w, img_c = img.shape\n",
    "    for n in range(N_CROP_TRIALS):\n",
    "        c_size = rand_between(300, max(img_h, img_w))\n",
    "        x0 = rand_between(0, max(0, img_w - c_size))\n",
    "        y0 = rand_between(0, max(0, img_h - c_size))\n",
    "        x1 = min(img_w, x0 + c_size)\n",
    "        y1 = min(img_h, y0 + c_size)\n",
    "        crop_products = products_df[(products_df.file == img_file) & \n",
    "                                 (products_df.xmin > x0) & (products_df.xmax < x1) &\n",
    "                                 (products_df.ymin > y0) & (products_df.ymax < y1)]\n",
    "        if len(crop_products) == 0:\n",
    "            continue\n",
    "        crop_img_file = f'{img_file[:-4]}{x0}_{y0}_{x1}_{y1}.JPG'\n",
    "        crop = img[y0:y1, x0:x1]\n",
    "        h, w, c = crop.shape\n",
    "        ratio = min(CROP_SIZE/h, CROP_SIZE/w)\n",
    "        crop = cv2.resize(crop, (0,0), fx=ratio, fy=ratio)\n",
    "        crop = crop[0:CROP_SIZE, 0:CROP_SIZE]\n",
    "        h, w, c = crop.shape\n",
    "        # add crop inner products to train_products or eval_products list\n",
    "        for xmin, ymin, xmax, ymax in \\\n",
    "                crop_products[['xmin', 'ymin', 'xmax', 'ymax']].values:\n",
    "            xmin -= x0\n",
    "            xmax -= x0\n",
    "            ymin -= y0\n",
    "            ymax -= y0\n",
    "\n",
    "            xmin, xmax, ymin, ymax = [int(np.round(e * ratio)) \n",
    "                                      for e in [xmin, xmax, ymin, ymax]]\n",
    "            product = {'filename': crop_img_file, 'class':'pack', \n",
    "                       'width':w, 'height':h,\n",
    "                       'xmin':xmin, 'ymin':ymin, 'xmax':xmax, 'ymax':ymax}\n",
    "            if is_train:\n",
    "                train_products.append(product)\n",
    "            else:\n",
    "                eval_products.append(product)\n",
    "        subpath = ['eval/', 'train/'][is_train]\n",
    "        cv2.imwrite(f'{cropped_path}{subpath}{crop_img_file}', crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_products).set_index('filename')\n",
    "eval_df = pd.DataFrame(eval_products).set_index('filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'pack':\n",
    "        return 1\n",
    "    else:\n",
    "        None\n",
    "\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) \n",
    "            for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_records(images_path, examples, dst_file):\n",
    "    writer = tf.io.TFRecordWriter(dst_file)\n",
    "    grouped = split(examples, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, images_path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_tf_records(f'{cropped_path}train/', train_df, f'{detector_data_path}train.record')\n",
    "convert_to_tf_records(f'{cropped_path}eval/', eval_df, f'{detector_data_path}eval.record')#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>shelf_id</th>\n",
       "      <th>planogram_id</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C4_P08_N2_S2_1.JPG</td>\n",
       "      <td>C4_P08</td>\n",
       "      <td>N2_S2_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1_P12_N1_S2_1.JPG</td>\n",
       "      <td>C1_P12</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C2_P07_N2_S2_1.JPG</td>\n",
       "      <td>C2_P07</td>\n",
       "      <td>N2_S2_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C4_P02_N4_S2_1.JPG</td>\n",
       "      <td>C4_P02</td>\n",
       "      <td>N4_S2_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C3_P03_N1_S3_1.JPG</td>\n",
       "      <td>C3_P03</td>\n",
       "      <td>N1_S3_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file shelf_id planogram_id  is_train\n",
       "0  C4_P08_N2_S2_1.JPG   C4_P08      N2_S2_1     False\n",
       "1  C1_P12_N1_S2_1.JPG   C1_P12      N1_S2_1     False\n",
       "2  C2_P07_N2_S2_1.JPG   C2_P07      N2_S2_1     False\n",
       "3  C4_P02_N4_S2_1.JPG   C4_P02      N4_S2_1     False\n",
       "4  C3_P03_N1_S3_1.JPG   C3_P03      N1_S3_1     False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load photos dataframe to get all evaluation images names\n",
    "testing_df = training_df[~training_df.is_train]\n",
    "testing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load frozen graph\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.io.gfile.GFile(PATH_TO_MODEL, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shubhamsingh/opt/anaconda3/lib/python3.7/site-packages/object_detection/utils/label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load categories (we have only 1 category pack)\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(\n",
    "    label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write function that executes detection\n",
    "def run_inference_for_single_image(image, image_tensor, sess, tensor_dict):\n",
    "    # Run inference\n",
    "    expanded_dims = np.expand_dims(image, 0)\n",
    "    output_dict = sess.run(tensor_dict, feed_dict={image_tensor: expanded_dims})\n",
    "    # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "    output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
    "    output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "    output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is useful to be able to run inference not only on the whole image,\n",
    "# but also on its parts\n",
    "# cutoff - minimum detection scrore needed to take box\n",
    "def run_inference_for_image_part(image_tensor, sess, tensor_dict, \n",
    "                                 image, cutoff, ax0, ay0, ax1, ay1):\n",
    "    boxes = []\n",
    "    im = image[ay0:ay1, ax0:ax1]\n",
    "    h, w, c = im.shape\n",
    "    output_dict = run_inference_for_single_image(im, image_tensor, sess, tensor_dict)\n",
    "    for i in range(100):\n",
    "        if output_dict['detection_scores'][i] < cutoff:\n",
    "            break\n",
    "        y0, x0, y1, x1, score = *output_dict['detection_boxes'][i], \\\n",
    "                                output_dict['detection_scores'][i]\n",
    "        x0, y0, x1, y1, score = int(x0*w), int(y0*h), \\\n",
    "                                int(x1*w), int(y1*h), \\\n",
    "                                int(score * 100)\n",
    "        boxes.append((x0+ax0, y0+ay0, x1+ax0, y1+ay0, score))\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_image_part_pcnt(image_tensor, sess, tensor_dict, \n",
    "                                 image, cutoff, p_ax0, p_ay0, p_ax1, p_ay1):\n",
    "    h, w, c = image.shape\n",
    "    max_x, max_y = w-1, h-1\n",
    "    return run_inference_for_image_part(\n",
    "                                image_tensor, sess, tensor_dict, \n",
    "                                image, cutoff, \n",
    "                                int(p_ax0*max_x), int(p_ay0*max_y), \n",
    "                                int(p_ax1*max_x), int(p_ay1*max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display image with bounding boxes\n",
    "def display_image_with_boxes(image, boxes, file, p_x0=0, p_y0=0, p_x1=1, p_y1=1):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    for x0, y0, x1, y1, score in boxes:\n",
    "        image = cv2.rectangle(image, (x0, y0), (x1, y1), (0,255,0), 5)\n",
    "    if p_x0 != 0 or p_y0 !=0 or p_x1 != 1 or p_y1 != 1:\n",
    "        h, w, c = image.shape\n",
    "        max_x, max_y = w-1, h-1\n",
    "        image = cv2.rectangle(image, \n",
    "                              (int(p_x0*max_x), int(p_y0*max_y)), \n",
    "                              (int(p_x1*max_x), int(p_y1*max_y)), (0,0,255), 5)\n",
    "    cv2.imwrite(f'{output_image_path}{file}', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializations function\n",
    "def initialize_graph():\n",
    "    ops = tf.get_default_graph().get_operations()\n",
    "    all_tensor_names = {output.name\n",
    "                        for op in ops\n",
    "                        for output in op.outputs}\n",
    "    tensor_dict = {}\n",
    "    for key in ['num_detections', 'detection_boxes',\n",
    "                'detection_scores', 'detection_classes',\n",
    "                'detection_masks']:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "            tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "    return image_tensor, tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for non-maximum suppression\n",
    "def non_max_suppression(boxes, overlapThresh):\n",
    "    if len(boxes) == 0:\n",
    "        return np.array([]).astype(\"int\")\n",
    "\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    " \n",
    "    pick = []\n",
    "\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    sc = boxes[:,4]\n",
    " \n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(sc)\n",
    " \n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    " \n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    " \n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        #todo fix overlap-contains...\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "         \n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "    \n",
    "    return boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sliding_window_inference_with_nm_suppression(file, cutoff):\n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            image_tensor, tensor_dict = initialize_graph()\n",
    "            image = cv2.imread(f'{PATH_TO_IMAGES}{file}')\n",
    "            h, w, c = image.shape\n",
    "            boxes = run_inference_for_image_part_pcnt(\n",
    "                image_tensor, sess, tensor_dict, image, cutoff, 0, 0, 1, 1)\n",
    "            a = np.array(boxes)\n",
    "            mean_dx = int(np.mean(a[:,2]-a[:,0]))\n",
    "            mean_dy = int(np.mean(a[:,3]-a[:,1]))\n",
    "            step_x, step_y = mean_dx, mean_dy\n",
    "            window_size = 2*mean_dy\n",
    "            boxes = []\n",
    "            y0 = 0\n",
    "            while y0 < h-1:\n",
    "                x0 = 0\n",
    "                while x0 < w-1:\n",
    "                    x1, y1 = x0 + window_size, y0 + window_size\n",
    "                    boxes += run_inference_for_image_part(\n",
    "                        image_tensor, sess, tensor_dict, image, cutoff, \n",
    "                        x0, y0, x1, y1)\n",
    "                    x0 += step_y\n",
    "                y0 += step_x\n",
    "            boxes = non_max_suppression(np.array(boxes), 0.5)\n",
    "            display_image_with_boxes(image, boxes, file)\n",
    "            return len(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={}\n",
    "annotation_df = pd.read_csv(f'{PATH_TO_ANNOTATION}', header = None)\n",
    "for test_file in testing_df.file:\n",
    "    boxes = do_sliding_window_inference_with_nm_suppression(test_file, 0.7)\n",
    "    data[test_file] = boxes\n",
    "writeToJSONFile(f'{data_path}{\"pack_detector\"}', \"image2products\", data)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Files are Removed!\n"
     ]
    }
   ],
   "source": [
    "#removing output file so that don't get any error at runtime\n",
    "\n",
    "import os\n",
    "try:\n",
    "    os.remove(f'{absolute_path}{\"pack_detector/image2products.json\"}')\n",
    "    os.remove(f'{absolute_path}{\"pack_detector/tmp.json\"}')\n",
    "    os.remove(f'{absolute_path}{\"pack_detector/metrics.json\"}')\n",
    "    print(\"All Files are Removed!\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = classification_report(y_validation, y_validation_score, [0,1,2,3,4,5,6,7,8,9], digits=2, output_dict=True)\n",
    "writeToJSONFile(f'{data_path}{\"pack_detector\"}', \"tmp\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrix={}\n",
    "average_precision=[0]*10\n",
    "with open(f'{data_path}{\"pack_detector/\"}{\"tmp.json\"}', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "    for i in range(num_classes):\n",
    "        average_precision[i] = average_precision_score(y_validation[:, i], y_validation_score[:, i])\n",
    "        matrix[str(i)]={\"mAP\": average_precision[i].tolist(), \"precision\" : data[str(i)]['precision'], \"recall\" : data[str(i)]['recall']}\n",
    "    writeToJSONFile(f'{data_path}{\"pack_detector\"}', \"metrics\", matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
